<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly | Openinx Blog</title>
<meta name=keywords content="Database,HBase,JVM"><meta name=description content="In HBASE-21879, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap ByteBuffers directly, while before HBASE-21879 we just read the HFileBlock to heap which would still lead to high GC pressure.
After few months of development and testing, all subtasks have been resovled now except the HBASE-21946 (It depends on HDFS-14483 and our HDFS teams are working on this, we expect the HDFS-14483 to be included in hadoop 2."><meta name=author content="Zheng Hu"><link rel=canonical href=https://openinx.github.io/posts/2019-06-23-offheap-block-reading/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://openinx.github.io/posts/2019-06-23-offheap-block-reading/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B52L98PJKS"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B52L98PJKS")}</script><meta property="og:url" content="https://openinx.github.io/posts/2019-06-23-offheap-block-reading/"><meta property="og:site_name" content="Openinx Blog"><meta property="og:title" content="Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly"><meta property="og:description" content="In HBASE-21879, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap ByteBuffers directly, while before HBASE-21879 we just read the HFileBlock to heap which would still lead to high GC pressure.
After few months of development and testing, all subtasks have been resovled now except the HBASE-21946 (It depends on HDFS-14483 and our HDFS teams are working on this, we expect the HDFS-14483 to be included in hadoop 2."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-23T13:27:08+08:00"><meta property="article:modified_time" content="2019-06-23T13:27:08+08:00"><meta property="article:tag" content="Database"><meta property="article:tag" content="HBase"><meta property="article:tag" content="JVM"><meta property="og:image" content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly"><meta name=twitter:description content="In HBASE-21879, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap ByteBuffers directly, while before HBASE-21879 we just read the HFileBlock to heap which would still lead to high GC pressure.
After few months of development and testing, all subtasks have been resovled now except the HBASE-21946 (It depends on HDFS-14483 and our HDFS teams are working on this, we expect the HDFS-14483 to be included in hadoop 2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://openinx.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly","item":"https://openinx.github.io/posts/2019-06-23-offheap-block-reading/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly","name":"Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly","description":"In HBASE-21879, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap ByteBuffers directly, while before HBASE-21879 we just read the HFileBlock to heap which would still lead to high GC pressure.\nAfter few months of development and testing, all subtasks have been resovled now except the HBASE-21946 (It depends on HDFS-14483 and our HDFS teams are working on this, we expect the HDFS-14483 to be included in hadoop 2.","keywords":["Database","HBase","JVM"],"articleBody":"In HBASE-21879, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap ByteBuffers directly, while before HBASE-21879 we just read the HFileBlock to heap which would still lead to high GC pressure.\nAfter few months of development and testing, all subtasks have been resovled now except the HBASE-21946 (It depends on HDFS-14483 and our HDFS teams are working on this, we expect the HDFS-14483 to be included in hadoop 2.9.3 and after that the HBASE-21946 will get resolved). we think the feature is stable enough now.\nWe have designed 3 test cases to prove the performance improvment with HBASE-21879:\nDisabled BlockCache, which means the cacheHitRatio is 0%; CacheHitRatio~65%; CachehHitRatio~100%; In our performance results[4], we can see that: the case#1 have an great performance improvement (throughput increased about 17%, heap allocation decreased about 95%, Young generaion size decreased about 81.7%), that’s because after HBASE-21879 all reads will allocate from pooled offheap bytebuffers and almost no heap allocation, while before HBASE-21879 the read path will create so many heap allocations. On the other hand, from the testing results of case#2 and case#3 we can also see that: As the cacheHitRatio increasing, the difference between before-HBASE-21879 and after-HBASE-21879 will decrease, when cacheHitRatio is 100%, they almost have no much difference in both throughput and latency.\nFor more details please see the document. Thanks Anoop/Ram/DuoZhang/Stack/GuanghaoZhang very much for your meticulous work (Suggession, discussion, patch reviewing, doc reviewing etc).\n","wordCount":"238","inLanguage":"en","image":"https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2019-06-23T13:27:08+08:00","dateModified":"2019-06-23T13:27:08+08:00","author":{"@type":"Person","name":"Zheng Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://openinx.github.io/posts/2019-06-23-offheap-block-reading/"},"publisher":{"@type":"Organization","name":"Openinx Blog","logo":{"@type":"ImageObject","url":"https://openinx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://openinx.github.io/ accesskey=h title="openinx (Alt + H)">openinx</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://openinx.github.io/about/ title=About><span>About</span></a></li><li><a href=https://openinx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://openinx.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://openinx.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://openinx.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Further GC optimization for HBase3.x: Reading HFileBlock into offheap directly</h1><div class=post-meta><span title='2019-06-23 13:27:08 +0800 +0800'>June 23, 2019</span>&nbsp;·&nbsp;Zheng Hu</div></header><div class=post-content><p>In <a href=https://issues.apache.org/jira/browse/HBASE-21879>HBASE-21879</a>, we redesigned the offheap read path: read the HFileBlock from HDFS to pooled offheap
ByteBuffers directly, while before <a href=https://issues.apache.org/jira/browse/HBASE-21879>HBASE-21879</a> we just read the HFileBlock to heap which would still lead
to high GC pressure.</p><p>After few months of development and testing, all subtasks have been resovled now except the <a href=https://issues.apache.org/jira/browse/HBASE-21946>HBASE-21946</a>
(It depends on <a href=https://issues.apache.org/jira/browse/HDFS-14483>HDFS-14483</a> and our HDFS teams are working on this, we expect the HDFS-14483 to be included
in hadoop 2.9.3 and after that the HBASE-21946 will get resolved). we think the feature is stable enough now.</p><p>We have designed 3 test cases to prove the performance improvment with HBASE-21879:</p><ol><li>Disabled BlockCache, which means the cacheHitRatio is 0%;</li><li>CacheHitRatio~65%;</li><li>CachehHitRatio~100%;</li></ol><p>In our performance results[4], we can see that: the case#1 have an great performance improvement
(throughput increased about 17%, heap allocation decreased about 95%, Young generaion size decreased
about 81.7%), that&rsquo;s because after HBASE-21879 all reads will allocate from pooled offheap bytebuffers
and almost no heap allocation, while before HBASE-21879 the read path will create so many heap allocations.
On the other hand, from the testing results of case#2 and case#3 we can also see that: As the cacheHitRatio
increasing, the difference between before-HBASE-21879 and after-HBASE-21879 will decrease, when
cacheHitRatio is 100%, they almost have no much difference in both throughput and latency.</p><p>For more details please see the <a href=/ppt/HBASE-21879-document.pdf>document</a>. Thanks Anoop/Ram/DuoZhang/Stack/GuanghaoZhang very much for your meticulous work (Suggession, discussion, patch reviewing, doc reviewing etc).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://openinx.github.io/tags/database/>Database</a></li><li><a href=https://openinx.github.io/tags/hbase/>HBase</a></li><li><a href=https://openinx.github.io/tags/jvm/>JVM</a></li></ul><nav class=paginav><a class=prev href=https://openinx.github.io/posts/2019-06-29-nebula-meetup/><span class=title>« Prev</span><br><span>Nebula北京Meetup总结</span>
</a><a class=next href=https://openinx.github.io/posts/2019-02-23-netty-memory-management/><span class=title>Next »</span><br><span>从HBase offheap到Netty的内存管理</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://openinx.github.io/>Openinx Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>