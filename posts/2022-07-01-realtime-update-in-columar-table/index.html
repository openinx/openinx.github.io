<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SQLServer的列存更新方案 | Openinx Blog</title>
<meta name=keywords content="Database,Storage Engine,Transaction"><meta name=description content="最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得  行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 毫秒级列存更新 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。
论文主要讲了 4 个方面的话题：

内存中实现列存表；
对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新；
对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描；
计算层在列存表的扫描性能上做了哪些优化。

第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。
其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 “TP为主，AP为辅” 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 点查 和 小范围 查询的行存索引，使得这个表可以完成 “AP为主，TP为辅” 的混合QUERY。
&ldquo;TP为主，AP为辅“ 的索引方案

SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 写入延迟 和 读取延迟 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。"><meta name=author content="Zheng Hu"><link rel=canonical href=https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B52L98PJKS"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B52L98PJKS")}</script><meta property="og:url" content="https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/"><meta property="og:site_name" content="Openinx Blog"><meta property="og:title" content="SQLServer的列存更新方案"><meta property="og:description" content="最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得 行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 毫秒级列存更新 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。
论文主要讲了 4 个方面的话题：
内存中实现列存表； 对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新； 对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描； 计算层在列存表的扫描性能上做了哪些优化。 第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。
其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 “TP为主，AP为辅” 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 点查 和 小范围 查询的行存索引，使得这个表可以完成 “AP为主，TP为辅” 的混合QUERY。
“TP为主，AP为辅“ 的索引方案 SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 写入延迟 和 读取延迟 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-01T12:00:24+08:00"><meta property="article:modified_time" content="2022-07-01T12:00:24+08:00"><meta property="article:tag" content="Database"><meta property="article:tag" content="Storage Engine"><meta property="article:tag" content="Transaction"><meta property="og:image" content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="SQLServer的列存更新方案"><meta name=twitter:description content="最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得  行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 毫秒级列存更新 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。
论文主要讲了 4 个方面的话题：

内存中实现列存表；
对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新；
对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描；
计算层在列存表的扫描性能上做了哪些优化。

第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。
其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 “TP为主，AP为辅” 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 点查 和 小范围 查询的行存索引，使得这个表可以完成 “AP为主，TP为辅” 的混合QUERY。
&ldquo;TP为主，AP为辅“ 的索引方案

SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 写入延迟 和 读取延迟 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://openinx.github.io/posts/"},{"@type":"ListItem","position":2,"name":"SQLServer的列存更新方案","item":"https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SQLServer的列存更新方案","name":"SQLServer的列存更新方案","description":"最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得 行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 毫秒级列存更新 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。\n论文主要讲了 4 个方面的话题：\n内存中实现列存表； 对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新； 对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描； 计算层在列存表的扫描性能上做了哪些优化。 第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。\n其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 “TP为主，AP为辅” 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 点查 和 小范围 查询的行存索引，使得这个表可以完成 “AP为主，TP为辅” 的混合QUERY。\n\u0026ldquo;TP为主，AP为辅“ 的索引方案 SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 写入延迟 和 读取延迟 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。\n","keywords":["Database","Storage Engine","Transaction"],"articleBody":"最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得 行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 毫秒级列存更新 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。\n论文主要讲了 4 个方面的话题：\n内存中实现列存表； 对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新； 对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描； 计算层在列存表的扫描性能上做了哪些优化。 第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。\n其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 “TP为主，AP为辅” 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 点查 和 小范围 查询的行存索引，使得这个表可以完成 “AP为主，TP为辅” 的混合QUERY。\n“TP为主，AP为辅“ 的索引方案 SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 写入延迟 和 读取延迟 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。\n读取的时候，每一个 RowGroup 内的数据先要过一遍 DeleteBitMap，然后再去 Delete Buffer 中做 点查 。读取路径上，扫描可以以 RowGroup 为单位做并发扫描，对于非常大的冷数据，都可以走 DeleteBitMap 过滤掉被删除的数据，极少数的热数据，会需要做一次 key compare 的 Delete Buffer 点查，这地方会增加一些耗时。当总体来说，还是比较好地处理了读写延迟以及冷热数据的问题。简而言之，就是大体量的冷数据对耗时贡献的少一些，少量热数据对耗时贡献多一点，这使得总体来说比较平衡。\n问题 1：对于大数据里面的纯列存表是否适合采用这种更新方案？\n假设我现在要实现一个类似 Kudu 这种纯实时更新的列存方案，我的结论是选择用 SQL Server 这种方案是不恰当的。\n首先，大数据里面的写入只有 UPSERT 和 DELETE 两种。通常意义上，大数据里面带key的 INSERT 可以认为就是 UPSERT 语义，这个语义表示如果 key 存在，那么就用覆盖掉老的值；如果这个 key 不存在，就直接插入新的值；而 UPDATE 又可以认为是先 DELETE 然后再 INSERT。那么既然大数据里面的所有写入，都是 UPSERT，这就会导致所有的写入操作都会在 Delete Buffer 中写一条行来提前删除这个row，再写一条新数据到 Delta Store。注意：这里 Delete Buffer 的体量是等价于整个大数据的写入体量。这直接导致 Delete Buffer 会非常非常的大，这就会带来一系列的副作用，例如扫描路径性能差，后台的Compaction任务压力非常大。\n但是，为什么 SQL Server 没有这个问题呢？因为 SQL Server 对这个索引的定位是 “TP为主，AP为辅”，它所有的写入请求都是先走一遍 TP 的行存索引，判断这个 key 是否存在，存在的话，直接报错给用户说 duplicate key，不存在的话，就直接写 INSERT。这样落到 AP 的列存索引这边来，就全部都是语义非常确定的 INSERT 操作了，而不是 UPSERT 操作。于是，在 SQL Server 里面的 Delete Buffer 是真正的 Delete，而不是由 UPSERT 产生的大量额外 DELETE。\n问题 2：能否搞定 Partial Update 的问题 ？\n所谓 Partial Update，就是指定主键以及部分column值进行更新。比如一个表有（pk, a, b) 三个字段，现在要求把 pk=1 的行对应的 b 值改成 3，更新的时候不提供 a 的值。\n在 SQL Server 里面同一个表既有行存 Index，又有列存 Index。所以，即使只提供了部分列的新列，其他的的列都可以从行存的 Index 中读出来。最后把这行数据的所有列的值都填上，直接写到列存 Index 中即可。\n但是，如果想在一个纯列存的表内搞定 Partial Update 这个问题，挑战其实还挺大的。如果采用这种标记 Delete Bitmap 的方式，那么意味着每次写入都必须写 整行所有的列值 ， 这意味着需要在列存文件中做一次整行读取，由于列存文件中同一行的每一个列都是分开存储的，做整行读取会消耗 O(n) 次 Random Seek，这个开销非常大，完全无法接受。现在我看到的较好的方案是 Apache Kudu 的方案，它每次在写入 Partial Update 的时候，直接把这个更新当作一个 Operation 记录到 Delta 内，相当于记了个 。这个 Changes 就是对应的 Operation。然后，读取的时候，需要把这个 Changes Apply 到 Delete_Row_ID 对应的 Row 上。相当于以牺牲部分读取性能的情况下，保证了写入的效率。\n“AP为主，TP为辅” 的索引方案 在 SQL Server 2014 的版本中，用来做数据分析的列存表，这个表上面是没有任何 Primary Key Index 、外键约束、二级索引的。所以，这个表只能用来服务分析型查询，一旦数据分析师想要去查具体的某个整行数据时，就必须对整表做一个全表扫描，才能找出这个行。为了解决这些问题，SQL Server 在 Columnar File 之上搞了一个 B-Tree 索引，这个 B-Tree 索引维护着 （这里 Row_ID 等于 ）的映射关系，这里的 Key 不一定是 Unique 的。这样用户需要根据 Key 查找整行数据的时候，就可以走一遍这个 B-Tree 定位到具体的 Row_ID （即上面说的 )，最后在把每个 Column 的 Value 都读出来即可。这里虽然依然是在 Columnar Store 里面做了一个点查，N 个 Column 可能产生 N 次 Random Seek，但考虑到这种请求很少，并且大部分热点数据都可以走 Cache。所以，对于 ”AP为主，TP为辅“ 的场景来说，这个索引设计基本上能满足需求。\n注意，对于同一份 Columnar File，SQL Server 是可以针对不同的 Key 建立多个不同的 B-Tree 索引的。这样使得不同的 Query 可以按照不同的 Key Index 去做 Point Lookup。对各种 Query 的支持都可以做到相对来说高效的响应。\nSQL Server的数据写入分两种，一种可以认为是单个 Txn 单独写入一行数据；另外一种可以认为是 Bulk Load，就是一次 Txn 写入大批量的数据。这两种写入方式，在流程上是不一样的。前者会先把单行数据写到 Delta Store 内，因为小体量的数据其实在 Cache 内攒一攒，最后一次性 Flush 到磁盘比较友好；后者是会直接开一个磁盘上的 RowGroup 做 Append 写，就不需要走 Delta Store 再 Flush 到磁盘了，因为对于大体量的数据写入，其实是没有必要来污染系统的 Cache 了。\n这里有几个问题需要考虑：\n问题 1：因为 Insert Row 会先走 Delta Store ，等 Delta Store 攒满一批数据之后，再 Flush 到 Columnar File 内。那么，最开始 B-Tree 中存的 的这个 Row_ID 指向的是 Delta Store 中生成的 Row_ID，等 Flush 完成之后，Row_ID 就应该指向磁盘文件中的压缩后的 Row Group 的 Row_ID。这意味着所有的用 B-Tree 构建的二级索引，都需要更新这个 变动 的 RowGroup 内所有 Key 的 Row_ID。这会对整个系统产生巨大的压力。\nSQL Server 的解决思路是维护了一个 Mapping Index ，这个 Mapping Index 背后也是可以 B-Tree 索引，维护着 原始 Row_ID 和 新 Row_ID 的映射关系，其中 B-Tree 里面的 Key 就是 原始 Row_ID，而 Value 就是 新 Row_ID。考虑到 Delta Store 中的每一次 RowGroup 变动，都是以 RowGroup 为单位的，所以 B-Tree 内部可以直接记录 Row_Group_ID 的映射即可，所以我认为 B-Tree 的 Key 其实可以直接是 Row_Group_ID，而不需要存 Row_ID 了。\n问题 2：在 Columnar File 中做数据删除时，如何删除二级索引 B-Tree 中的信息 ？（假设不删 B-Tree 二级索引的信息，则后面的 Query 通过 B-Tree 做点查依然有会读取到已经删除的 Row )\n由于 B-Tree 中记录的 Row_ID 可能是 原始 Row_ID ，而这个 原始 Row_ID 对应的 Row 可能已经经过 RowGroup Move，变成了 新的Row_ID。所以，SQL Server 设计的时候，在每一个Row中新增了一个 Column，这个 Column 用来 记录这个 Row 的原始 Row_ID，这样去 B-Tree 中做删除的时候，直接用 Key 和 原始 Row_ID 去删除即可。（注意，这里 Key 不一定 Unique，所以删除的时候一定需要明确删除同一个 Key 的具体 原始 Row_ID ）。\n问题 3：如何实现数据的点查 ？\n可以参考上图。按以下情况分类：\n如果通过 B-Tree 找到的 Row_ID 落在 Delta Store 内，那么直接返回即可； 如果通过 B-Tree 找到的 Row_ID 落在一个 Invalid 的 RowGroup上，那么需要走 Mapping Index 找到这个 Invalid Row Group 最新的 RowGroup 是啥； 如果通过 B-Tree 知道的 Row_ID 落在一个 Valid 的磁盘的 Row Group上，直接去这个 Row Group 查找即可。 References [1] http://www.vldb.org/pvldb/vol8/p1740-Larson.pdf\n[2] https://kudu.apache.org/kudu.pdf\n[3] http://www.odbms.org/wp-content/uploads/2014/07/PositionalDelat-Trees.pdf\n","wordCount":"557","inLanguage":"en","image":"https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2022-07-01T12:00:24+08:00","dateModified":"2022-07-01T12:00:24+08:00","author":{"@type":"Person","name":"Zheng Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/"},"publisher":{"@type":"Organization","name":"Openinx Blog","logo":{"@type":"ImageObject","url":"https://openinx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://openinx.github.io/ accesskey=h title="openinx (Alt + H)">openinx</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://openinx.github.io/about/ title=About><span>About</span></a></li><li><a href=https://openinx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://openinx.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://openinx.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://openinx.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">SQLServer的列存更新方案</h1><div class=post-meta><span title='2022-07-01 12:00:24 +0800 +0800'>July 1, 2022</span>&nbsp;·&nbsp;Zheng Hu</div></header><div class=post-content><p>最近读了一下 Microsoft 在 2015 年 VLDB 上发表的论文 《Real-Time Analytical Processing with SQL Server》[1]。SQLServer算是业内较早实现并落地 HTAP 行列更新方案的产品，我其实觉得 行存（Row-Wise Index） 在OLTP场景下的各种设计以及取舍，大家都已经讨论的非常清楚了。对于高效率的 <strong>毫秒级列存更新</strong> 方案，业界有一些方案设计和实现，典型的如Kudu[2]、Positional Delta Tree[3] 等，但远不如行存讨论的多。我仔细的 Review 了一下 SQLServer 的技术方案，觉得他们的设计还是颇具参考意义。本文分享一下我对这篇文章的一些看法。</p><p>论文主要讲了 4 个方面的话题：</p><ul><li>内存中实现列存表；</li><li>对于行存表，怎么实现一个列存索引结构，从而实现分析型 SQL 的极大提速。这部分的重点就是怎么实现列存二级索引的实时更新；</li><li>对于列存表怎么实现一个 B-Tree 的二级索引，使得这个列存表可以用来做点查和小范围扫描；</li><li>计算层在列存表的扫描性能上做了哪些优化。</li></ul><p>第1部分纯内存的列存表相对比较简单，第4部分的列存表扫描在业界的很多论文中都可以找到类似的方案。第2部分和第3部分相对比较独特，可以说是整篇文章的精华，也是我比较感兴趣的部分。</p><p>其实第2部分，可以认为是在一个已经存在的OLTP表上，怎么来创建一个OLAP的列存索引，使得这个表可以跑 <strong>“TP为主，AP为辅”</strong> 的两种QUERY；而第3部分正好完全相反，相当于是在一个纯粹的OLAP列存表之上，创建一个适合做 <strong>点查</strong> 和 <strong>小范围</strong> 查询的行存索引，使得这个表可以完成 <strong>“AP为主，TP为辅”</strong> 的混合QUERY。</p><h3 id=tp为主ap为辅-的索引方案>&ldquo;TP为主，AP为辅“ 的索引方案<a hidden class=anchor aria-hidden=true href=#tp为主ap为辅-的索引方案>#</a></h3><p><img alt="SQL Server Columnar Secondary Index" loading=lazy src=/images/sql-server-columnar-secondary-index.png></p><p>SQL Server的这种 CSI （Columnar Store Index）更新方案，说实话其实在 <strong>写入延迟</strong> 和 <strong>读取延迟</strong> 上处理的还算比较恰当。对于列存索引来说，如果是 INSERT 的话，那么直接 Append 到 Delta Store 里面就行，没啥其他的额外耗时操作；如果是 DELETE 的话，先看一下 DELETE 的 key 是否在 Delta Store 里面，如果在就直接删除即可；如果不在，就插入一条 Delete Marker 到这个由 B-Tree 索引实现的 Delete Buffer。无论是 INSERT 还是 DELETE 操作，都没有特别耗时的地方。可能往 B-Tree 索引实现的 Delete Buffer 里面写 Delete Marker 会有点儿耗时，但这个 Delete Buffer 一般都是最近更新比较热的数据。首先体量不会特别大，其次，大部分的都是热数据，都会被 Cache 到内存。然后，后台有一个任务， 会默默地把 Delete Buffer 里面的 Delete 操作都转换成 Delete Bitmap。</p><p>读取的时候，每一个 RowGroup 内的数据先要过一遍 DeleteBitMap，然后再去 Delete Buffer 中做 <strong>点查</strong> 。读取路径上，扫描可以以 <strong>RowGroup</strong> 为单位做并发扫描，对于非常大的冷数据，都可以走 DeleteBitMap 过滤掉被删除的数据，极少数的热数据，会需要做一次 key compare 的 Delete Buffer 点查，这地方会增加一些耗时。当总体来说，还是比较好地处理了读写延迟以及冷热数据的问题。简而言之，就是大体量的冷数据对耗时贡献的少一些，少量热数据对耗时贡献多一点，这使得总体来说比较平衡。</p><p><strong>问题 1</strong>：对于大数据里面的纯列存表是否适合采用这种更新方案？</p><p>假设我现在要实现一个类似 Kudu 这种纯实时更新的列存方案，我的结论是选择用 SQL Server 这种方案是不恰当的。</p><p>首先，大数据里面的写入只有 <strong>UPSERT</strong> 和 <strong>DELETE</strong> 两种。通常意义上，大数据里面带key的 <strong>INSERT</strong> 可以认为就是 <strong>UPSERT</strong> 语义，这个语义表示如果 key 存在，那么就用覆盖掉老的值；如果这个 key 不存在，就直接插入新的值；而 <strong>UPDATE</strong> 又可以认为是先 <strong>DELETE</strong> 然后再 <strong>INSERT</strong>。那么既然大数据里面的所有写入，都是 UPSERT，这就会导致所有的写入操作都会在 Delete Buffer 中写一条行来提前删除这个row，再写一条新数据到 Delta Store。注意：这里 Delete Buffer 的体量是等价于整个大数据的写入体量。这直接导致 Delete Buffer 会非常非常的大，这就会带来一系列的副作用，例如扫描路径性能差，后台的Compaction任务压力非常大。</p><p>但是，为什么 SQL Server 没有这个问题呢？因为 SQL Server 对这个索引的定位是 “TP为主，AP为辅”，它所有的写入请求都是先走一遍 TP 的行存索引，判断这个 key 是否存在，存在的话，直接报错给用户说 duplicate key，不存在的话，就直接写 INSERT。这样落到 AP 的列存索引这边来，就全部都是语义非常确定的 <strong>INSERT</strong> 操作了，而不是 <strong>UPSERT</strong> 操作。于是，在 SQL Server 里面的 Delete Buffer 是真正的 Delete，而不是由 UPSERT 产生的大量额外 DELETE。</p><p><strong>问题 2</strong>：能否搞定 Partial Update 的问题 ？</p><p>所谓 Partial Update，就是指定主键以及部分column值进行更新。比如一个表有（pk, a, b) 三个字段，现在要求把 pk=1 的行对应的 b 值改成 3，更新的时候不提供 a 的值。</p><p>在 SQL Server 里面同一个表既有行存 Index，又有列存 Index。所以，即使只提供了部分列的新列，其他的的列都可以从行存的 Index 中读出来。最后把这行数据的所有列的值都填上，直接写到列存 Index 中即可。</p><p>但是，如果想在一个纯列存的表内搞定 Partial Update 这个问题，挑战其实还挺大的。如果采用这种标记 Delete Bitmap 的方式，那么意味着每次写入都必须写 <strong>整行所有的列值</strong> ， 这意味着需要在列存文件中做一次整行读取，由于列存文件中同一行的每一个列都是分开存储的，做整行读取会消耗 O(n) 次 Random Seek，这个开销非常大，完全无法接受。现在我看到的较好的方案是 Apache Kudu 的方案，它每次在写入 Partial Update 的时候，直接把这个更新当作一个 Operation 记录到 Delta 内，相当于记了个 <code>&lt;Delete_Row_ID, Changes></code> 。这个 Changes 就是对应的 Operation。然后，读取的时候，需要把这个 <code>Changes</code> Apply 到 <code>Delete_Row_ID</code> 对应的 Row 上。相当于以牺牲部分读取性能的情况下，保证了写入的效率。</p><h3 id=ap为主tp为辅-的索引方案>“AP为主，TP为辅” 的索引方案<a hidden class=anchor aria-hidden=true href=#ap为主tp为辅-的索引方案>#</a></h3><p>在 SQL Server 2014 的版本中，用来做数据分析的列存表，这个表上面是没有任何 Primary Key Index 、外键约束、二级索引的。所以，这个表只能用来服务分析型查询，一旦数据分析师想要去查具体的某个整行数据时，就必须对整表做一个全表扫描，才能找出这个行。为了解决这些问题，SQL Server 在 Columnar File 之上搞了一个 B-Tree 索引，这个 B-Tree 索引维护着 <code>&lt;Key, Row_ID></code> （这里 Row_ID 等于 <code>&lt;Row_Group_ID, Offset_In_RowGroup></code> ）的映射关系，这里的 Key 不一定是 Unique 的。这样用户需要根据 Key 查找整行数据的时候，就可以走一遍这个 B-Tree 定位到具体的 Row_ID （即上面说的 <code>&lt;Row_Group_ID, Offset_In_RowGroup></code> )，最后在把每个 Column 的 Value 都读出来即可。这里虽然依然是在 Columnar Store 里面做了一个点查，N 个 Column 可能产生 N 次 Random Seek，但考虑到这种请求很少，并且大部分热点数据都可以走 Cache。所以，对于 ”AP为主，TP为辅“ 的场景来说，这个索引设计基本上能满足需求。</p><p>注意，对于同一份 Columnar File，SQL Server 是可以针对不同的 Key 建立多个不同的 B-Tree 索引的。这样使得不同的 Query 可以按照不同的 Key Index 去做 Point Lookup。对各种 Query 的支持都可以做到相对来说高效的响应。</p><p><img alt="SQL Server B-Tree Index in a columar index" loading=lazy src=/images/sqlserver-b-tree-index-in-columar-index.png></p><p>SQL Server的数据写入分两种，一种可以认为是单个 Txn 单独写入一行数据；另外一种可以认为是 Bulk Load，就是一次 Txn 写入大批量的数据。这两种写入方式，在流程上是不一样的。前者会先把单行数据写到 Delta Store 内，因为小体量的数据其实在 Cache 内攒一攒，最后一次性 Flush 到磁盘比较友好；后者是会直接开一个磁盘上的 RowGroup 做 Append 写，就不需要走 Delta Store 再 Flush 到磁盘了，因为对于大体量的数据写入，其实是没有必要来污染系统的 Cache 了。</p><p>这里有几个问题需要考虑：</p><p><strong>问题 1</strong>：因为 Insert Row 会先走 Delta Store ，等 Delta Store 攒满一批数据之后，再 Flush 到 Columnar File 内。那么，最开始 B-Tree 中存的 <code>&lt;Key, Row_ID></code> 的这个 Row_ID 指向的是 Delta Store 中生成的 Row_ID，等 Flush 完成之后，Row_ID 就应该指向磁盘文件中的压缩后的 Row Group 的 Row_ID。这意味着所有的用 B-Tree 构建的二级索引，都需要更新这个 <strong>变动</strong> 的 RowGroup 内所有 Key 的 Row_ID。这会对整个系统产生巨大的压力。</p><p>SQL Server 的解决思路是维护了一个 Mapping Index ，这个 Mapping Index 背后也是可以 B-Tree 索引，维护着 <em>原始 Row_ID</em> 和 <em>新 Row_ID</em> 的映射关系，其中 B-Tree 里面的 Key 就是 <em>原始 Row_ID</em>，而 Value 就是 <em>新 Row_ID</em>。考虑到 Delta Store 中的每一次 RowGroup 变动，都是以 RowGroup 为单位的，所以 B-Tree 内部可以直接记录 Row_Group_ID 的映射即可，所以我认为 B-Tree 的 Key 其实可以直接是 Row_Group_ID，而不需要存 Row_ID 了。</p><p><strong>问题 2</strong>：在 Columnar File 中做数据删除时，如何删除二级索引 B-Tree 中的信息 ？（假设不删 B-Tree 二级索引的信息，则后面的 Query 通过 B-Tree 做点查依然有会读取到已经删除的 Row )</p><p>由于 B-Tree 中记录的 Row_ID 可能是 <em>原始 Row_ID</em> ，而这个 <em>原始 Row_ID</em> 对应的 Row 可能已经经过 RowGroup Move，变成了 <em>新的Row_ID</em>。所以，SQL Server 设计的时候，在每一个Row中新增了一个 Column，这个 Column 用来 <strong>记录这个 Row 的原始 Row_ID</strong>，这样去 B-Tree 中做删除的时候，直接用 Key 和 <em>原始 Row_ID</em> 去删除即可。（注意，这里 Key 不一定 Unique，所以删除的时候一定需要明确删除同一个 Key 的具体 <em>原始 Row_ID</em> ）。</p><p><strong>问题 3</strong>：如何实现数据的点查 ？</p><p>可以参考上图。按以下情况分类：</p><ul><li>如果通过 B-Tree 找到的 Row_ID 落在 Delta Store 内，那么直接返回即可；</li><li>如果通过 B-Tree 找到的 Row_ID 落在一个 Invalid 的 RowGroup上，那么需要走 Mapping Index 找到这个 Invalid Row Group 最新的 RowGroup 是啥；</li><li>如果通过 B-Tree 知道的 Row_ID 落在一个 Valid 的磁盘的 Row Group上，直接去这个 Row Group 查找即可。</li></ul><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] <a href=http://www.vldb.org/pvldb/vol8/p1740-Larson.pdf>http://www.vldb.org/pvldb/vol8/p1740-Larson.pdf</a><br>[2] <a href=https://kudu.apache.org/kudu.pdf>https://kudu.apache.org/kudu.pdf</a><br>[3] <a href=http://www.odbms.org/wp-content/uploads/2014/07/PositionalDelat-Trees.pdf>http://www.odbms.org/wp-content/uploads/2014/07/PositionalDelat-Trees.pdf</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://openinx.github.io/tags/database/>Database</a></li><li><a href=https://openinx.github.io/tags/storage-engine/>Storage Engine</a></li><li><a href=https://openinx.github.io/tags/transaction/>Transaction</a></li></ul><nav class=paginav><a class=prev href=https://openinx.github.io/posts/2025-04-10-iceberg-summit-2025-1/><span class=title>« Prev</span><br><span>Iceberg Summit 2025 - Part 1</span>
</a><a class=next href=https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/><span class=title>Next »</span><br><span>Pingcap Hackthon2019</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://openinx.github.io/>Openinx Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>