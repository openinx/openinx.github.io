<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pingcap Hackthon2019 | Openinx Blog</title>
<meta name=keywords content="Database,Storage Engine"><meta name=description content="最近参加了TiDB的Hackthon2019比赛，一直都想写一篇总结，现在总算有点时间来写一下。
这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。
我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：
第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。
第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。
第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。

我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。
我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。"><meta name=author content="Zheng Hu"><link rel=canonical href=https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B52L98PJKS"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B52L98PJKS")}</script><meta property="og:url" content="https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/"><meta property="og:site_name" content="Openinx Blog"><meta property="og:title" content="Pingcap Hackthon2019"><meta property="og:description" content="最近参加了TiDB的Hackthon2019比赛，一直都想写一篇总结，现在总算有点时间来写一下。
这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。
我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：
第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。
第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。
第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。
我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。
我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-10-26T13:16:43+08:00"><meta property="article:modified_time" content="2019-10-26T13:16:43+08:00"><meta property="article:tag" content="Database"><meta property="article:tag" content="Storage Engine"><meta property="og:image" content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Pingcap Hackthon2019"><meta name=twitter:description content="最近参加了TiDB的Hackthon2019比赛，一直都想写一篇总结，现在总算有点时间来写一下。
这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。
我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：
第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。
第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。
第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。

我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。
我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://openinx.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Pingcap Hackthon2019","item":"https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pingcap Hackthon2019","name":"Pingcap Hackthon2019","description":"最近参加了TiDB的Hackthon2019比赛，一直都想写一篇总结，现在总算有点时间来写一下。\n这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。\n我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：\n第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。\n第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。\n第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。\n我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。\n我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。\n","keywords":["Database","Storage Engine"],"articleBody":"最近参加了TiDB的Hackthon2019比赛，一直都想写一篇总结，现在总算有点时间来写一下。\n这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。\n我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：\n第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。\n第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。\n第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。\n我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。\n我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。\n我们觉得有了这三个 Case 的性能数据，已经足够说明 io_uring 对 TiDB 的性能价值了。\n比赛的第一天，我们基本完成了用 io_uring 重写 RocksDB 写 WAL 流程。但是跑了下 benchmark 发现，不用 io_uring 优化和用 io_uring 优化的RocksDB写入性能基本差不多。后面看了下，有个bug。修复之后，再测了一把发现io_uring优化后的性能远远好于没优化的状态。这是当时测试出来的数据，明显可以看出来优化后write+fsync的带宽比未优化快了一倍。\nmaster: fillrandom : 77.439 micros/op 12913 ops/sec; 1.4 MB/s uring : fillrandom : 36.503 micros/op 27394 ops/sec; 3.0 MB/s 看到这个数据，直觉告诉我们这肯定是不对的。因为毕竟io_uring主要优化批量io的吞吐，现在我们只改写了write+sync差距不应该有一倍之巨。于是，我们开始各种review代码，再反复读io_uring 17页的技术文档。最后，决定写个简单的小程序来确认io_uring接口和native的write+sync差距到底有多大，注意，这其实是定位问题最常用的思路，就是通过排除法缩小问题的范围，例如这里如果我们排除了io_uring接口本身造成的巨大差距，那么马上就可以定位是Linux接口上层的问题，而不是Linux接口本身的问题。测了一下，数据显示io_uring接口本身比Linux write+sync接口快10%左右。这就说明是我们上层写错了，导致之前测试结果的巨大差距。\n花了一点时间查了下，发现是由于代码每次直接查了下队首的complete event而没有出队列，这就相当于write+sync的时候，sync其实就没有卡在那里等。于是uring优化write+sync的代码写成write+write的代码，这带宽当然比未优化要好太多。 改了这个bug之后，又发现压测RocksDB老是crash。\n于是我们又开始翻RocksDB写入路径的各种代码以及Linux API的文档。结果还是没有找到root cause，“但后面又想了下这个结果肯定不行，最后决定再仔细缕一下每一行代码。最后我提了几段review comment之后，跟吴毅讨论某一段代码，吴毅突然想起改的Pull Request里那个io_uring_prep_write(seq, fd_, iov, 1, filesize_)，其中iov是个局部变量，内存会随着函数跑完而释放，导致io_uring_prep_write这个异步方法访问到一个被释放掉的内存地址，于是最终coredump。\n简单来说，就是这么一个场景：\nvoid run(){ int[100] iov; //... io_uring_prep_write(seq, fd_, iov, 1, filesize_); //... //end of run function() } 由于io_uring_prep_write是异步执行的，所以在run函数中iov退出之后，io_uring_prep_write内部实现其实还需要访问iov内存，但是由于iov是个局部变量，所以退出了run()函数内存就被释放。于是造成了coredump。\n找到这个问题之后，修复了一下。最终测试出来的结果还是比较合理的。\n用uring优化之后能提升3%的ops，这里效果不是特别明显，但是足以说明io_uring的优化效果。至于不明显的原因，我们想了下可能有这么几点：\n1.测试的数据量较小且测试路径没开O_DIRECT，所以数据都cache在page cache中了。也就是说这个3%的性能提升，基本上来自linux内核代码实现本身的优化，而不是批量磁盘设备IO带来的收益。这其实在大数据量下（或者走O_DIRECT写入）是能看出差异的。\n2.批量IO太少，这里我们只有write+sync两个IO操作是异步的（如下图所示），所以其实多次write+sync还是同步的，效果肯定不会有太大的提升。\n当然，后续我们还针对MultiRead和Compaction两种场景做了异步话，性能也有不同程度的提升，这里不再赘述。感兴趣的朋友可以参考我们最后答辩的PPT。\n虽说这个项目最后并没有拿到什么奖，但是我们觉得这是一次很好的性能优化实践，未来TiKV也会把io_uring作为他们的技术储备，这也是我们乐于看到的。\n再谈一下对这个比赛的看法。去年比赛总共有80+人参加，基本上20只队伍，感觉今年人气旺了很多，有120+人参加，共有40+只队伍，整个比赛评审一直持续到晚上9点多，我们这些参赛者都不得不在路上看直播评审（毕竟得回家赶高铁）。另外一个感受是，这种比赛对于上层的项目更加友好，而对于底层的项目相对不友好，记得有个组演示了下在浏览器里面跑TiDB，在浏览器里通过SQL终端连上服务端执行show databases那一刻，底下一群观众在不断鼓掌，我就觉得他们这个组肯定能拿个大奖。毕竟在整个技术栈中，从上层到下层，能理解的受众逐级递减，到了我们优化Linux API的这波人，底下基本没几个愿意听的。演示6分钟会相对吃亏，同时底层的性能优化短期内也难以收到很明显的成效，这些都是做底层项目的劣势吧，所以给组委会的一个建议就是需要更多的鼓励一下底层demo实现，否则未来的Hackthon会有更多的团队选择做上层甚至前端项目，而不愿意做底层的性能优化。\n最后，我个人在这个比赛中可以全身心的投入写代码和解决问题，还是玩的挺开心的。感谢PingCAP提供的平台，让我认识了一群有趣的人。\n","wordCount":"157","inLanguage":"en","image":"https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2019-10-26T13:16:43+08:00","dateModified":"2019-10-26T13:16:43+08:00","author":{"@type":"Person","name":"Zheng Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://openinx.github.io/posts/2019-10-26-pingcap-hackthon2019/"},"publisher":{"@type":"Organization","name":"Openinx Blog","logo":{"@type":"ImageObject","url":"https://openinx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://openinx.github.io/ accesskey=h title="openinx (Alt + H)">openinx</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://openinx.github.io/about/ title=About><span>About</span></a></li><li><a href=https://openinx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://openinx.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://openinx.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://openinx.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Pingcap Hackthon2019</h1><div class=post-meta><span title='2019-10-26 13:16:43 +0800 +0800'>October 26, 2019</span>&nbsp;·&nbsp;Zheng Hu</div></header><div class=post-content><p>最近参加了TiDB的<a href=https://pingcap.com/community-cn/hackathon2019/>Hackthon2019</a>比赛，一直都想写一篇总结，现在总算有点时间来写一下。</p><p>这是一个围绕 TiDB 分布式数据库展开的一个编程比赛，在 48 小时内完成一个可以展示的 demo，并在 6 分钟内向评委说明项目思路及展示demo。理论上，要求这个demo从实用性、易用性、性能三个方面来优化TiDB（占评分40%），“当然作品的完成度也是一个很重要的考核方向（占30%)，其他就是创新性（占20%）和展示度了（占10%）。</p><p>我们队共有三个同学：队长是来自 PingCAP 美国 office 的吴毅同学，他之前在 facebook 负责 RocksDB 的研发，目前在PingCAP 主要负责 RocksDB 的研发和性能优化，另外一个北京 office 的张博康同学，负责TiKV的研发。由于我们队三个人都是做存储底层的研发，所以就想尝试在今年的 Hackthon 上做一些偏底层的工作。在比赛前，我们大致确认可能会尝试的几个小方向：</p><p>第一个是做一套 TiKV 的分布式 trace，用来跟进一个 TiKV 请求整个生命周期的耗时情况，便于诊断性能。由于这是一个非常普遍的方向，我们预估可能会有不少团队跟我们撞车，另外印象中现在 TiKV 已经实现了部分工作，所以就没有选择这个方向。</p><p>第二个是把一个图查询引擎套在 TiKV 之上，实现一个图数据库。我们初步想好可以用这个图数据库展示社交网络中的三度人脉。在不需要我们开发前端的前提下，可以借助开源的图查询前端来展示 demo，至少演示上不会吃亏。这个其实是一个不错的候选，但不确定工作量有多大，最后也没有选择这个方向。</p><p>第三个是用最新 linux 引入的高性能纯异步实现的 IO 接口 liburing 来重写部分 rocksdb 的实现，期望能给 TiKV 带来更好的性能提升。这个课题看起来跟我们三个成员的背景比较匹配，于是最终我们选择了这个课题。</p><p><img alt="TiDB Hackton2019 iouring" loading=lazy src=/images/tidb-hackthon2019-iouring.png></p><p>我们队吴毅和博康选择在北京 office 比赛，而我离上海比较近，所以就去了上海。为此还申请了异地组队权限（感谢下PingCAP 开放的组委会），我们应该是唯一的三人两地的队伍了。</p><p>我们大致阅读了 liburing 的技术文档，大致确定可以尝试用这套异步 IO 接口重写 RocksDB 的写 WAL 流程和Compaction 流程。另外也调研到 facebook 之前已经尝试过用 io_uring 重写 RocksDB 的 MultiRead 实现，发现随机读的 IOPS 能翻三倍，接口延迟也下降不少，所以我们想用 TiKV 的一个场景来说明可以从中获得性能收益。</p><p><img alt="TiDB Hackton2019 RocksDB multiread Perf" loading=lazy src=/images/tidb-hackthon2019-rocksdb-multiread-perf.png></p><p>我们觉得有了这三个 Case 的性能数据，已经足够说明 io_uring 对 TiDB 的性能价值了。</p><p>比赛的第一天，我们基本完成了用 io_uring 重写 RocksDB 写 WAL 流程。但是跑了下 benchmark 发现，不用 io_uring 优化和用 io_uring 优化的RocksDB写入性能基本差不多。后面看了下，有个bug。修复之后，再测了一把发现io_uring优化后的性能远远好于没优化的状态。这是当时测试出来的数据，明显可以看出来优化后write+fsync的带宽比未优化快了一倍。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nl>master</span><span class=p>:</span><span class=w> </span><span class=n>fillrandom</span><span class=w>   </span><span class=p>:</span><span class=w>      </span><span class=n>77</span><span class=p>.</span><span class=na>439</span><span class=w> </span><span class=n>micros</span><span class=o>/</span><span class=n>op</span><span class=w> </span><span class=n>12913</span><span class=w> </span><span class=n>ops</span><span class=o>/</span><span class=n>sec</span><span class=p>;</span><span class=w>    </span><span class=n>1</span><span class=p>.</span><span class=na>4</span><span class=w> </span><span class=n>MB</span><span class=o>/</span><span class=n>s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>uring</span><span class=w> </span><span class=p>:</span><span class=w> </span><span class=n>fillrandom</span><span class=w>   </span><span class=p>:</span><span class=w>      </span><span class=n>36</span><span class=p>.</span><span class=na>503</span><span class=w> </span><span class=n>micros</span><span class=o>/</span><span class=n>op</span><span class=w> </span><span class=n>27394</span><span class=w> </span><span class=n>ops</span><span class=o>/</span><span class=n>sec</span><span class=p>;</span><span class=w>    </span><span class=n>3</span><span class=p>.</span><span class=na>0</span><span class=w> </span><span class=n>MB</span><span class=o>/</span><span class=n>s</span><span class=w>
</span></span></span></code></pre></div><p>看到这个数据，直觉告诉我们这肯定是不对的。因为毕竟io_uring主要优化批量io的吞吐，现在我们只改写了write+sync差距不应该有一倍之巨。于是，我们开始各种review代码，再反复读io_uring 17页的技术文档。最后，决定写个简单的小程序来确认io_uring接口和native的write+sync差距到底有多大，注意，这其实是定位问题最常用的思路，就是通过排除法缩小问题的范围，例如这里如果我们排除了io_uring接口本身造成的巨大差距，那么马上就可以定位是Linux接口上层的问题，而不是Linux接口本身的问题。测了一下，<a href=https://github.com/PingCAP-Hackthon2019-Team17/liburing-demo>数据</a>显示io_uring接口本身比Linux write+sync接口快10%左右。这就说明是我们上层写错了，导致之前测试结果的巨大差距。</p><p>花了一点时间查了下，发现是由于代码每次直接查了下队首的complete event而没有出队列，这就相当于write+sync的时候，sync其实就没有卡在那里等。于是uring优化write+sync的代码写成write+write的代码，这带宽当然比未优化要好太多。
改了这个bug之后，又发现压测RocksDB老是crash。</p><p>于是我们又开始翻RocksDB写入路径的各种代码以及Linux API的文档。结果还是没有找到root cause，“但后面又想了下这个结果肯定不行，最后决定再仔细缕一下每一行代码。最后我提了几段review comment之后，跟吴毅讨论某一段代码，吴毅突然想起改的Pull Request里那个io_uring_prep_write(seq, fd_, iov, 1, filesize_)，其中iov是个局部变量，内存会随着函数跑完而释放，导致io_uring_prep_write这个异步方法访问到一个被释放掉的内存地址，于是最终coredump。</p><p>简单来说，就是这么一个场景：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=nf>run</span><span class=p>(){</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span><span class=p>[</span><span class=mi>100</span><span class=p>]</span> <span class=n>iov</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>io_uring_prep_write</span><span class=p>(</span><span class=n>seq</span><span class=p>,</span> <span class=n>fd_</span><span class=p>,</span> <span class=n>iov</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>filesize_</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//end of run function()
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>由于io_uring_prep_write是异步执行的，所以在run函数中iov退出之后，io_uring_prep_write内部实现其实还需要访问iov内存，但是由于iov是个局部变量，所以退出了run()函数内存就被释放。于是造成了coredump。</p><p>找到这个问题之后，修复了一下。最终测试出来的结果还是比较合理的。</p><p><img alt="TiDB Hackton2019 RocksDB Write Sync Perf" loading=lazy src=/images/tidb-hackthon2019-rocksdb-write-sync-perf.png></p><p>用uring优化之后能提升3%的ops，这里效果不是特别明显，但是足以说明io_uring的优化效果。至于不明显的原因，我们想了下可能有这么几点：</p><p>1.测试的数据量较小且测试路径没开O_DIRECT，所以数据都cache在page cache中了。也就是说这个3%的性能提升，基本上来自linux内核代码实现本身的优化，而不是批量磁盘设备IO带来的收益。这其实在大数据量下（或者走O_DIRECT写入）是能看出差异的。</p><p>2.批量IO太少，这里我们只有write+sync两个IO操作是异步的（如下图所示），所以其实多次write+sync还是同步的，效果肯定不会有太大的提升。</p><p><img alt="TiDB Hackton2019 Write Sync Flow" loading=lazy src=/images/tidb-hackthon2019-write-sync-flow.png></p><p>当然，后续我们还针对MultiRead和Compaction两种场景做了异步话，性能也有不同程度的提升，这里不再赘述。感兴趣的朋友可以参考我们最后<a href=/ppt/io-uring.pdf>答辩的PPT</a>。</p><p>虽说这个项目最后并没有拿到什么奖，但是我们觉得这是一次很好的性能优化实践，未来TiKV也会把io_uring作为他们的技术储备，这也是我们乐于看到的。</p><p>再谈一下对这个比赛的看法。去年比赛总共有80+人参加，基本上20只队伍，感觉今年人气旺了很多，有120+人参加，共有40+只队伍，整个比赛评审一直持续到晚上9点多，我们这些参赛者都不得不在路上看直播评审（毕竟得回家赶高铁）。另外一个感受是，这种比赛对于上层的项目更加友好，而对于底层的项目相对不友好，记得有个组演示了下在浏览器里面跑TiDB，在浏览器里通过SQL终端连上服务端执行show databases那一刻，底下一群观众在不断鼓掌，我就觉得他们这个组肯定能拿个大奖。毕竟在整个技术栈中，从上层到下层，能理解的受众逐级递减，到了我们优化Linux API的这波人，底下基本没几个愿意听的。演示6分钟会相对吃亏，同时底层的性能优化短期内也难以收到很明显的成效，这些都是做底层项目的劣势吧，所以给组委会的一个建议就是需要更多的鼓励一下底层demo实现，否则未来的Hackthon会有更多的团队选择做上层甚至前端项目，而不愿意做底层的性能优化。</p><p>最后，我个人在这个比赛中可以全身心的投入写代码和解决问题，还是玩的挺开心的。感谢PingCAP提供的平台，让我认识了一群有趣的人。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://openinx.github.io/tags/database/>Database</a></li><li><a href=https://openinx.github.io/tags/storage-engine/>Storage Engine</a></li></ul><nav class=paginav><a class=prev href=https://openinx.github.io/posts/2022-07-01-realtime-update-in-columar-table/><span class=title>« Prev</span><br><span>SQLServer的列存更新方案</span>
</a><a class=next href=https://openinx.github.io/posts/2019-09-10-tuning-the-hbase-2-write-performance/><span class=title>Next »</span><br><span>一场HBase2.x的写入性能优化之旅</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://openinx.github.io/>Openinx Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>