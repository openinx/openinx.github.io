<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>HBaseCon West 2017 Session解读 | Openinx Blog</title><meta name=keywords content="HBase,Apache"><meta name=description content="HBaseCon West 2017的PPT解读如下：
1. HBase at Xiaomi
由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：
1. 0.94升级到0.98集群的一些经验。
2. 小米内部HBase使用g1gc的一些经验。
3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。
2. Apache HBase at DiDi (by Kang Yuan)
主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行GeoHash转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。
3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)
有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。
按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：
由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。 ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。 对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。 最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。
ps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。 另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408
4. Efficient and portable data processing with Apache Beam and HBase (By Google)
这个演讲更多是来HBaseCon宣传下Apache Beam这个项目。 Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。"><meta name=author content="Me"><link rel=canonical href=https://openinx.github.io/posts/2017-06-28-hbaseconwest2017/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B52L98PJKS"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B52L98PJKS",{anonymize_ip:!1})}</script><meta property="og:title" content="HBaseCon West 2017 Session解读"><meta property="og:description" content="HBaseCon West 2017的PPT解读如下：
1. HBase at Xiaomi
由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：
1. 0.94升级到0.98集群的一些经验。
2. 小米内部HBase使用g1gc的一些经验。
3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。
2. Apache HBase at DiDi (by Kang Yuan)
主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行GeoHash转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。
3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)
有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。
按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：
由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。 ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。 对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。 最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。
ps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。 另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408
4. Efficient and portable data processing with Apache Beam and HBase (By Google)
这个演讲更多是来HBaseCon宣传下Apache Beam这个项目。 Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。"><meta property="og:type" content="article"><meta property="og:url" content="https://openinx.github.io/posts/2017-06-28-hbaseconwest2017/"><meta property="og:image" content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-06-28T13:27:08+08:00"><meta property="article:modified_time" content="2017-06-28T13:27:08+08:00"><meta property="og:site_name" content="Openinx Blog."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="HBaseCon West 2017 Session解读"><meta name=twitter:description content="HBaseCon West 2017的PPT解读如下：
1. HBase at Xiaomi
由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：
1. 0.94升级到0.98集群的一些经验。
2. 小米内部HBase使用g1gc的一些经验。
3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。
2. Apache HBase at DiDi (by Kang Yuan)
主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行GeoHash转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。
3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)
有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。
按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：
由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。 ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。 对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。 最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。
ps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。 另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408
4. Efficient and portable data processing with Apache Beam and HBase (By Google)
这个演讲更多是来HBaseCon宣传下Apache Beam这个项目。 Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://openinx.github.io/posts/"},{"@type":"ListItem","position":3,"name":"HBaseCon West 2017 Session解读","item":"https://openinx.github.io/posts/2017-06-28-hbaseconwest2017/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"HBaseCon West 2017 Session解读","name":"HBaseCon West 2017 Session解读","description":"HBaseCon West 2017的PPT解读如下：\n1. HBase at Xiaomi\n由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：\n1. 0.94升级到0.98集群的一些经验。\n2. 小米内部HBase使用g1gc的一些经验。\n3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。\n2. Apache HBase at DiDi (by Kang Yuan)\n主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行GeoHash转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。\n3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)\n有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。\n按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：\n由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。 ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。 对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。 最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。\nps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。 另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408\n4. Efficient and portable data processing with Apache Beam and HBase (By Google)\n这个演讲更多是来HBaseCon宣传下Apache Beam这个项目。 Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。","keywords":["HBase","Apache"],"articleBody":"HBaseCon West 2017的PPT解读如下：\n1. HBase at Xiaomi\n由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：\n1. 0.94升级到0.98集群的一些经验。\n2. 小米内部HBase使用g1gc的一些经验。\n3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。\n2. Apache HBase at DiDi (by Kang Yuan)\n主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行GeoHash转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。\n3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)\n有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。\n按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：\n由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。 ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。 对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。 最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。\nps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。 另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408\n4. Efficient and portable data processing with Apache Beam and HBase (By Google)\n这个演讲更多是来HBaseCon宣传下Apache Beam这个项目。 Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。\n5. Data Product at Airbnb\n来自Airbnb的这份演讲，主要介绍了Airbnb内部HBase的应用场景，以及如何实现统一执行batch任务和streaming任务。\n6. Democratizing HBase(by Hortonworks)\n作者Josh Elser是来自Hortonworks的工程师，也是HBase Committer，他参与了多个Apache的顶级项目，例如HBase/Phoenix等等。演讲主要介绍了目前HBase在多租户资源隔离方面的一些工作，主要包括安全认证，RPC Quotas，Rpc优先级，Space Quotas，RegionServer Group等内容。\n7. Apache Spark – Apache HBase Connector Feature Rich and Efficient Access to HBase through Spark SQL (by Hortonworks)\n介绍Hortonworks开源的SHC项目相关内容。\n8. Gohbase: Pure Go HBase Client(by Arista Networks)\n一般非Java语言(Python/Golang/Javascript)会采用Thrift协议生成各自语言的SDK, SDK先访问HBase的Thrift server，然后thrift server后端通过Java Native Client连RS/Master，通过ThriftServer中转来实现通信，但通过thrift协议生成的非Java版本的客户端接口比较原始，不是特别好用，另外早期的ThriftServer bug也比较多。\n因此，演讲者基于HBase的Protobuf协议，实现了一套纯Golang写的hbase client，其实相当于说把Java Native Client的逻辑全部用Golang写了一遍。使用体验应该要比其他Golang语言写的SDK好用。性能上，Gohbase某些场景下甚至优于Java的客户端。\n最后，作者在开发Gohbase的过程中，发现了HBASE-18066这个bug, 这个bug目前已经由小米openinx修复。这个bug的问题在于：设置closestRowBefore为true的Get操作，在RegionServer端的代码实现中，是先通过不带mvcc的方式拿到rowKey， 然后再通过带mvcc的方式去找这个rowKey对应的Value, 并返回。由于前者不带mvcc, 后者带mvcc，所以导致拿到的数据可能不一致。修复方式就是把这类操作都由reversed scan来实现，由于scan操作统一设了mvcc，也就解决了这个BUG。\n9. Analyzing Cryptocurrencies on HBase For Finance, Forensics, and Fraud (by Ripple)\nRipple是一种有点类似Bitcoin的加密数字货币，背后应该是有Ripple这个商业公司在支撑，这份演讲主要介绍了Ripple现状以及HBase在公司内部的应用场景，重点应该还是Ripple的相关业务介绍。\n10. Splice Machine: Powering Hybrid Applications (by Splice Machine)\nSplice Machine是一家做数据库(DAAS)云服务的厂商，PPT主要是演示了一遍他们的Hadoop相关云产品使用过程，对HBase云产品形态感兴趣的同学可以看一下。本次分享主要还是推广他们的产品。\n11. OpenTSDB: 2.4 and 3.0 update(by Yahoo)\n作者是一位维护OpenTSDB近4年的程序员，干货比较多。OpenTSDB是一款基于HBase之上开发的时间序列数据库产品，能方便的实现水平扩展。具体到HBase的存储时，OpenTSDB设计的RowKey为\n这样的RowKey设计，可以满足用户根据metric + ts + 若干个tag组合进行查询，用SQL来表达就是\nselect * from table where metric=? and ts \u003c ? and ts \u003e ? and (tag_i= ?) and (tag_j= ?)\n注意，如果一个查询的时间跨度较大，例如查24小时的监控数据。那么，这24小时的数据会分散在24个不同的行上，也就是说需要进行24次Seek+Read操作才能读取到完整的监控数据。因此，长跨度的查询必须在损失精度的情况下抽样，才能保证较好的性能。\nHBASE-15181 实现了DateTieredCompaction。DateTieredCompaction的策略是按照时间轴(一般待合并区间为[now - maxAge, now], 其中maxAge可配置) ，从新到旧划分时间区间，其中时间越新，划分时间区间长度越短，然后对时间轴内区间的多个文件做合并，落在对应区间的KeyValue被写入到对应区间的storeFile内，最终合并的结果是每个区间只有一个文件，这个文件只包含自己区间内的数据。这种策略的好处在于数据越新，compaction越频繁，数据越旧，compaction越少(甚至不compaction)，这种策略非常适合OpenTSDB，因为OpenTSDB写入数据都是按照时间递增，而之前的老数据从不修改，因此推荐OpenTSDB下的HBase集群采用DateTieredCompaction策略。\n另外，PPT中还提到很多其他的内容，例如AsyncHBase的进展，OpenTSDB3.0的规划，OpenTSDB2.4的其他新特性等等，这里不再展开细评， 感兴趣可以读读PPT。\n参考资料：\nhttps://labs.spotify.com/2014/12/18/date-tiered-compaction/ 12. A study of Salesforce’s use of HBase and Phoenix ( by Salesforce )\n主要介绍了Salesforce公司内部的HBase/Phoenix业务情况。\n13. Cursors in Apache Phoenix (by bloomberg)\nBloomberg是一家研发NewSQL的数据库厂商，其核心产品是comdb2，目前已经在Github开源，其项目的介绍专门在VLDB2016上发了paper，感兴趣的同学可以读读。本次talk主要介绍他们是如何在phoenix上实现的关系型数据库中cursor功能。\n14. Warp 10 - A novel approach for time series management and analysis based on HBase (by Herberts)\n这个PPT主要应该是介绍他们的Wrap10项目。\n15. Removable Singularity: A Story of HBaseUpgrade at Pinterest(by Pinterest)\nPinterest之前有40多个0.94版本的集群，他们面临着和小米一样的问题，就是需要把0.94版本的集群升级到1.2版本，而0.94版本HBase的rpc协议和1.2不兼容，前者是基于Writable接口的自定义协议，后者是Protobuf协议。解决问题的思路大致是一样的，就是先把快照数据通过ExportSnapshot导入1.2版本的HBase集群，再把增量数据通过thrift replication导入到新集群。后面需要考虑的问题就是业务如何实现在线切换。Pinterest比较有优势的一个地方就是，他们的业务使用的客户端是AsyncHBase1.7，而AsyncHBase1.7本身是支持0.94和1.2的，因此业务只需要修改访问HBase的集群地址就好。升级到1.2版本之后，无论是读性能还是写性能都有较好的提升。\n16. Improving HBase availability in a multi-tenant environment (by Hubspot)\nHBase在CAP中属于CP型的系统，可用性方面有一些缺陷，例如一个RegionServer宕机，Failover时长主要分3段，首先是故障检测的时间（\u003c10s），然后是SplitLog的时间(20~40s)，然后是最终RegionOnline的时间(10s~30s)，数据取自Hubspot PPT。总体来讲，Failover时长优化空间较大。为此，Hubspot分享了一些提高HBase可用性的相关work：\n测试发现将大集群拆分成更多小实例能减少总体的failover时间。 通过Normalizer控制Region的大小，使之均匀。 HBASE-17707和HBASE-18164优化StochasticLoadBalancer策略，使balance更合理，更高效。 对每个用户消耗的RPC Handler做上限限制，社区版本貌似没这功能，应该是内部实现的。 使用内部硬件检测工具提前发现硬件问题，并移走HBase服务。 17. Highly Available HBase (by Sift Science)\nSift Science是一家通过实时机器学习技术来防止商业诈骗的公司, Uber/Airbnb等都是他们的客户。由于他们的业务跟钱相关，对HBase可用性要求非常高。经过一些优化之后， 他们成功将HBase的宕机时间从5小时/月优化到4分钟/月。这些优化工作主要有：\n修改hbase-client代码，当client发现突然有较多请求失败时，client直接上抛DoNotRetryRegionException到上层，避免RS已经不可用时仍然被客户端大量请求导致大量handler被卡住，影响MTTR时间。 搭建在线备份集群，同时修改client代码，发现主集群有问题， client可以直接借助zk自动切换到备份集群。 存在的问题是，备份集群数据可能落后主集群，切换可能数据不一致。PPT上只是说会通过离线任务去抽样校验，所以，我觉得对于两集群的一致性问题，要么牺牲一致性保可用性，要么牺牲可用性保一致性。 加强监控，尤其是locality/balance状况/p99延迟这些，尽早发现问题并解决，而不是等出问题再解决。 18. Transactions in HBase (by cask.co)\n这个talk应该是全场21个talk中干货最多的PPT之一，讨论的重点是基于HBase之上如何实现分布式事务。作者深入探讨了3种分布式事务模型，分别是Tephra/Trafodion/Omid，3个项目都是Apache下的孵化项目。这里，简单说一下我对这3种事务模型的理解：\nTephra: 该模型本质上是通过一个叫做Trx Manager的集中式服务用来维护当前活跃事务，该TrxManager用来分配事务start/commit/rollback逻辑时钟，每个事务开始时都会分配一个ts和一个叫做excludes={…}的集合，用来维护该事务开始前的活跃事务，这个excludes集合可用来做隔离性检查，也可用来做冲突检查。由于有一个较为重量级的集中式TrxManager，所以事务高并发场景下该模型压力会较大，因此该模型推荐的用来运行并发少的long-running MapReduce之类的事务Job。 Trafodion: 这个没太看懂和tephra的区别是啥。。ppt上看不太出来。 Omid: Apache Omid事务模型和Google Percolator事务模型是类似的，小米研发的Themis也是一样，本质上是借助HBase的行级别事务来实现跨行跨表事务。每个事务begin和commit/rollback都需要去TimeOracle拿到一个全局唯一递增的逻辑timestamp，然后为每一个column新增两个隐藏的column分别叫做lock和write，用来跟踪每次写入操作的锁以及逻辑timestamp，通过2PC实现跨行事务提交。对于读取的4个隔离级别(RS, RR, SI, RU)，有了全局唯一递增的逻辑时钟之后，也就很容易决定哪些数据应该返回给用户。 参考资料\n1. http://andremouche.github.io/transaction/percolator.html\n2. https://research.google.com/pubs/pub36726.html\n3. https://github.com/xiaomi/themis\n4. http://nosqlmark.informatik.uni-hamburg.de/sdb2014/res/paper/Omid.pdf\n19. Achieving HBase Multi-Tenancy: RegionServer Groups and Favored Nodes\n这个talk介绍了Yahoos主导的两个重要feature：RegionServer Group和Favored Nodes。\n1. RegionServer Group可以让指定表分布在某些RegionServer上，一定程度上实现了多租户之间的隔离，相比Hortonworks在微观层面对多租户所做出的努力（rpc队列分离、quota机制、请求优先级等），个人认为rsGroup在多租户管理方面效果可能更加明显。rsGroup是这次峰会比较明星的话题，包括滴滴等公司都作为主旨来介绍其在生产实践中的应用，可见已经被很多大厂所接受并实践，遗憾的是，该功能目前只在未发布的2.0.0里面包含，要想在当前版本中使用必须移植对应patch，因为功能的复杂性，这看起来并不是一件容易的事情。\n2. Favored Nodes机制相对比较陌生，该功能允许HBase在region层面指定该region上所有文件分布在哪些DN之上，这个功能最大的好处是可以提升数据文件的本地率，试想，每个Region都知道文件存放的DN，迁移Region的时候就可以多考虑文件本地率进行迁移。\n20. Community-Driven Graphs With JanusGraph\n这个talk介绍了HBase应用的另一个重要领域－图数据库，图数据库在互联网业务中诸多方面（社交图谱分析、推荐分析）都有实际应用。比如大家可能比较熟悉的titan数据库就是一个典型的图数据库。而这个主题介绍的是另一个图数据库 - JanusGraph，PPT中分别对JanusGraph的架构、存储模型以及如何使用HBase作为后端存储进行了详细的分析。对Graph比较感兴趣的童鞋可以重点关注！\n最后，附上PPT下载链接: HBaseConWest2017\n作者：胡争、范欣欣\n","wordCount":"346","inLanguage":"en","datePublished":"2017-06-28T13:27:08+08:00","dateModified":"2017-06-28T13:27:08+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://openinx.github.io/posts/2017-06-28-hbaseconwest2017/"},"publisher":{"@type":"Organization","name":"Openinx Blog","logo":{"@type":"ImageObject","url":"https://openinx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://openinx.github.io accesskey=h title="openinx (Alt + H)">openinx</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://openinx.github.io/about/ title=About><span>About</span></a></li><li><a href=https://openinx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://openinx.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://openinx.github.io>Home</a>&nbsp;»&nbsp;<a href=https://openinx.github.io/posts/>Posts</a></div><h1 class=post-title>HBaseCon West 2017 Session解读</h1><div class=post-meta><span title='2017-06-28 13:27:08 +0800 CST'>June 28, 2017</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;346 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/openinx/blog/edit/main/content/posts/2017-06-28-hbaseconwest2017.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>HBaseCon West 2017的PPT解读如下：</p><p><strong>1. HBase at Xiaomi</strong></p><p>由小米的杨哲和张洸濠合作分享，两位是2016年新晋升的HBase Committer (ps: 小米目前总共产生了8位HBase Committer，其中2位HBase PMC，解决了数百个issue). 分享的一些亮点主要有：</p><p>1. 0.94升级到0.98集群的一些经验。<br>2. 小米内部HBase使用g1gc的一些经验。<br>3. 2016年小米对社区做的一些开发和改进，包括但不限于顺序推送复制日志/优化Scan操作/开发异步客户端功能以及相关测试结果，等等。</p><p><strong>2. Apache HBase at DiDi (by Kang Yuan)</strong></p><p>主要分享了HBase在滴滴的一些实践经验，目前滴滴的HBase是基于0.98.21版本，然后将rsgroup这个功能迁移到了自己的分支，用来做业务隔离。另外，PPT中也提到通过将地理位置坐标进行<a href=http://blog.jobbole.com/80633/>GeoHash</a>转换成一维byte存放到HBase中，可以解决查询一个点周边坐标的问题。</p><p><strong>3. Accordion: HBase Breathes with In-Memory Compaction (From Yahoo)</strong></p><p>有了InMemory-Compaction功能之后，HBase支持将Memstore直接Flush成一个ImmutableSegment，这个ImmutableSegment其实是一块内存空间，多次Memstore的Flush操作会导致产生多个ImmutableSegment，特定条件下，多个ImmtableSegment会进行In-Memory的Compaction，也就是多个ImmutableSegment完全在内存中合并成为一个大的ImmutableSegment（其中BASIC类型的InMemoryCompaction会保留所有数据，EAGER类型的InMemoryCompaction会清理冗余版本数据）。最终，这个大的ImmutableSegment还是要Flush到磁盘的，然后接着触发做磁盘上的Compaction操作。</p><p>按照设计文档以及PPT的说明，InMemory-Compaction有以下好处：</p><ul><li>由于InMemoryCompaction会在内存中进行compaction， 不用频繁的Flush Memstore到Disk(Flush次数太多会造成storefile个数增长, storefile的增长会导致读性能严重下降)， 从而可以降低读操作延迟。</li><li>ImmtableSegment今后可能会和HFile的layout保持一致，这样Flush的速度将大幅提升。</li><li>对于行数据频繁更新的场景，InMemory-Compaction可以采用EAGER方式在内存中就清理掉冗余版本数据，节省了这部分数据落盘的代价。</li></ul><p>最后，PPT测试数据也确实说明使用InMemoryCompaction后，写吞吐有较大幅度提升，读延迟有较大幅度下降。</p><p>ps. In-memory Compaction由stack等6位成员共同完成(将在HBase2.0的release版本发布），这其中有两位美女工程师（PPT中的照片证明颜值确实很高），现在都已经是HBase的Committer了。
另外，In-memory compaction详细设计文档请参考：https://issues.apache.org/jira/browse/HBASE-13408</p><p><strong>4. Efficient and portable data processing with Apache Beam and HBase (By Google)</strong></p><p>这个演讲更多是来HBaseCon宣传下<a href=https://beam.apache.org/>Apache Beam</a>这个项目。
Apache Beam这个项目始于2016年2月份，近1年多的时间内就收到了来自全球178个贡献者的8600+提交，主要是希望提供一个统一的API用来同时处理Batch任务和Streaming任务，他的API后端可以接Apex/Flink/Spark/GoogleCloudDataFlow等服务，同时提供Java和Python的客户端SDK。这个东西就好比JDBC一样，提供了一个统一的借口，后端可以连接MySQL/Oracle/Postgresql/SQLServer等关系型数据库。我没理解错的话，这个东西应该是可以用来在HBase/MongoDB/HDFS/Cassandra/Kafka/BigTable/Spanner/Elasticsearch/GridFS/Hive/AMQP等(超过20种通用的存储服务)各种服务间实现数据transform。</p><p><strong>5. Data Product at Airbnb</strong></p><p>来自Airbnb的这份演讲，主要介绍了Airbnb内部HBase的应用场景，以及如何实现统一执行batch任务和streaming任务。</p><p><strong>6. Democratizing HBase(by Hortonworks)</strong></p><p>作者Josh Elser是来自Hortonworks的工程师，也是HBase Committer，他参与了多个Apache的顶级项目，例如HBase/Phoenix等等。演讲主要介绍了目前HBase在多租户资源隔离方面的一些工作，主要包括安全认证，RPC Quotas，Rpc优先级，Space Quotas，RegionServer Group等内容。</p><p><strong>7. Apache Spark – Apache HBase Connector Feature Rich and Efficient Access to HBase through Spark SQL (by Hortonworks)</strong></p><p>介绍Hortonworks开源的<a href=https://github.com/hortonworks-spark/shc>SHC</a>项目相关内容。</p><p><strong>8. Gohbase: Pure Go HBase Client(by Arista Networks)</strong></p><p>一般非Java语言(Python/Golang/Javascript)会采用Thrift协议生成各自语言的SDK, SDK先访问HBase的Thrift server，然后thrift server后端通过Java Native Client连RS/Master，通过ThriftServer中转来实现通信，但通过thrift协议生成的非Java版本的客户端接口比较原始，不是特别好用，另外早期的ThriftServer bug也比较多。</p><p>因此，演讲者基于HBase的Protobuf协议，实现了一套纯Golang写的<a href=https://github.com/tsuna/gohbase>hbase client</a>，其实相当于说把Java Native Client的逻辑全部用Golang写了一遍。使用体验应该要比其他Golang语言写的SDK好用。性能上，Gohbase某些场景下甚至优于Java的客户端。</p><p>最后，作者在开发Gohbase的过程中，发现了<a href=https://issues.apache.org/jira/browse/HBASE-18066>HBASE-18066</a>这个bug, 这个bug目前已经由小米openinx修复。这个bug的问题在于：设置closestRowBefore为true的Get操作，在RegionServer端的代码实现中，是先通过不带mvcc的方式拿到rowKey， 然后再通过带mvcc的方式去找这个rowKey对应的Value, 并返回。由于前者不带mvcc, 后者带mvcc，所以导致拿到的数据可能不一致。修复方式就是把这类操作都由reversed scan来实现，由于scan操作统一设了mvcc，也就解决了这个BUG。</p><p><strong>9. Analyzing Cryptocurrencies on HBase For Finance, Forensics, and Fraud (by Ripple)</strong></p><p>Ripple是一种有点类似Bitcoin的加密数字货币，背后应该是有Ripple这个商业公司在支撑，这份演讲主要介绍了Ripple现状以及HBase在公司内部的应用场景，重点应该还是Ripple的相关业务介绍。</p><p><strong>10. Splice Machine: Powering Hybrid Applications (by Splice Machine)</strong></p><p>Splice Machine是一家做数据库(DAAS)云服务的厂商，PPT主要是演示了一遍他们的Hadoop相关云产品使用过程，对HBase云产品形态感兴趣的同学可以看一下。本次分享主要还是推广他们的产品。</p><p><strong>11. OpenTSDB: 2.4 and 3.0 update(by Yahoo)</strong></p><p>作者是一位维护OpenTSDB近4年的程序员，干货比较多。OpenTSDB是一款基于HBase之上开发的时间序列数据库产品，能方便的实现水平扩展。具体到HBase的存储时，OpenTSDB设计的RowKey为</p><p><img loading=lazy src=/images/opentsdb-rowkey.png alt=image></p><p>这样的RowKey设计，可以满足用户根据metric + ts + 若干个tag组合进行查询，用SQL来表达就是</p><p>select * from table where metric=? and ts &lt; ? and ts > ? and (tag_i= ?) and (tag_j= ?)</p><p>注意，如果一个查询的时间跨度较大，例如查24小时的监控数据。那么，这24小时的数据会分散在24个不同的行上，也就是说需要进行24次Seek+Read操作才能读取到完整的监控数据。因此，长跨度的查询必须在损失精度的情况下抽样，才能保证较好的性能。</p><p>HBASE-15181 实现了DateTieredCompaction。DateTieredCompaction的策略是按照时间轴(一般待合并区间为[now - maxAge, now], 其中maxAge可配置) ，从新到旧划分时间区间，其中时间越新，划分时间区间长度越短，然后对时间轴内区间的多个文件做合并，落在对应区间的KeyValue被写入到对应区间的storeFile内，最终合并的结果是每个区间只有一个文件，这个文件只包含自己区间内的数据。这种策略的好处在于数据越新，compaction越频繁，数据越旧，compaction越少(甚至不compaction)，这种策略非常适合OpenTSDB，因为OpenTSDB写入数据都是按照时间递增，而之前的老数据从不修改，因此推荐OpenTSDB下的HBase集群采用DateTieredCompaction策略。</p><p>另外，PPT中还提到很多其他的内容，例如AsyncHBase的进展，OpenTSDB3.0的规划，OpenTSDB2.4的其他新特性等等，这里不再展开细评， 感兴趣可以读读PPT。</p><p>参考资料：</p><ol><li><a href=https://labs.spotify.com/2014/12/18/date-tiered-compaction/>https://labs.spotify.com/2014/12/18/date-tiered-compaction/</a></li></ol><p><strong>12. A study of Salesforce’s use of HBase and Phoenix ( by Salesforce )</strong></p><p>主要介绍了Salesforce公司内部的HBase/Phoenix业务情况。</p><p><strong>13. Cursors in Apache Phoenix (by bloomberg)</strong></p><p>Bloomberg是一家研发NewSQL的数据库厂商，其核心产品是<a href=https://github.com/bloomberg/comdb2>comdb2</a>，目前已经在Github开源，其项目的介绍专门在VLDB2016上发了paper，感兴趣的同学可以读读。本次talk主要介绍他们是如何在phoenix上实现的关系型数据库中cursor功能。</p><p><strong>14. Warp 10 - A novel approach for time series management and analysis based on HBase (by Herberts)</strong></p><p>这个PPT主要应该是介绍他们的Wrap10项目。</p><p><strong>15. Removable Singularity: A Story of HBaseUpgrade at Pinterest(by Pinterest)</strong></p><p>Pinterest之前有40多个0.94版本的集群，他们面临着和小米一样的问题，就是需要把0.94版本的集群升级到1.2版本，而0.94版本HBase的rpc协议和1.2不兼容，前者是基于Writable接口的自定义协议，后者是Protobuf协议。解决问题的思路大致是一样的，就是先把快照数据通过ExportSnapshot导入1.2版本的HBase集群，再把增量数据通过thrift replication导入到新集群。后面需要考虑的问题就是业务如何实现在线切换。Pinterest比较有优势的一个地方就是，他们的业务使用的客户端是AsyncHBase1.7，而AsyncHBase1.7本身是支持0.94和1.2的，因此业务只需要修改访问HBase的集群地址就好。升级到1.2版本之后，无论是读性能还是写性能都有较好的提升。</p><p><strong>16. Improving HBase availability in a multi-tenant environment (by Hubspot)</strong></p><p>HBase在CAP中属于CP型的系统，可用性方面有一些缺陷，例如一个RegionServer宕机，Failover时长主要分3段，首先是故障检测的时间（&lt;10s），然后是SplitLog的时间(20~40s)，然后是最终RegionOnline的时间(10s~30s)，数据取自Hubspot PPT。总体来讲，Failover时长优化空间较大。为此，Hubspot分享了一些提高HBase可用性的相关work：</p><ul><li>测试发现将大集群拆分成更多小实例能减少总体的failover时间。</li><li>通过Normalizer控制Region的大小，使之均匀。</li><li>HBASE-17707和HBASE-18164优化StochasticLoadBalancer策略，使balance更合理，更高效。</li><li>对每个用户消耗的RPC Handler做上限限制，社区版本貌似没这功能，应该是内部实现的。</li><li>使用内部硬件检测工具提前发现硬件问题，并移走HBase服务。</li></ul><p><strong>17. Highly Available HBase (by Sift Science)</strong></p><p>Sift Science是一家通过实时机器学习技术来防止商业诈骗的公司, Uber/Airbnb等都是他们的客户。由于他们的业务跟钱相关，对HBase可用性要求非常高。经过一些优化之后， 他们成功将HBase的宕机时间从5小时/月优化到4分钟/月。这些优化工作主要有：</p><ul><li>修改hbase-client代码，当client发现突然有较多请求失败时，client直接上抛DoNotRetryRegionException到上层，避免RS已经不可用时仍然被客户端大量请求导致大量handler被卡住，影响MTTR时间。</li><li>搭建在线备份集群，同时修改client代码，发现主集群有问题， client可以直接借助zk自动切换到备份集群。 存在的问题是，备份集群数据可能落后主集群，切换可能数据不一致。PPT上只是说会通过离线任务去抽样校验，所以，我觉得对于两集群的一致性问题，要么牺牲一致性保可用性，要么牺牲可用性保一致性。</li><li>加强监控，尤其是locality/balance状况/p99延迟这些，尽早发现问题并解决，而不是等出问题再解决。</li></ul><p><strong>18. Transactions in HBase (by cask.co)</strong></p><p>这个talk应该是全场21个talk中干货最多的PPT之一，讨论的重点是基于HBase之上如何实现分布式事务。作者深入探讨了3种分布式事务模型，分别是Tephra/Trafodion/Omid，3个项目都是Apache下的孵化项目。这里，简单说一下我对这3种事务模型的理解：</p><ul><li>Tephra: 该模型本质上是通过一个叫做Trx Manager的集中式服务用来维护当前活跃事务，该TrxManager用来分配事务start/commit/rollback逻辑时钟，每个事务开始时都会分配一个ts和一个叫做excludes={&mldr;}的集合，用来维护该事务开始前的活跃事务，这个excludes集合可用来做隔离性检查，也可用来做冲突检查。由于有一个较为重量级的集中式TrxManager，所以事务高并发场景下该模型压力会较大，因此该模型推荐的用来运行并发少的long-running MapReduce之类的事务Job。</li><li>Trafodion: 这个没太看懂和tephra的区别是啥。。ppt上看不太出来。</li><li>Omid: Apache Omid事务模型和Google Percolator事务模型是类似的，小米研发的Themis也是一样，本质上是借助HBase的行级别事务来实现跨行跨表事务。每个事务begin和commit/rollback都需要去TimeOracle拿到一个全局唯一递增的逻辑timestamp，然后为每一个column新增两个隐藏的column分别叫做lock和write，用来跟踪每次写入操作的锁以及逻辑timestamp，通过2PC实现跨行事务提交。对于读取的4个隔离级别(RS, RR, SI, RU)，有了全局唯一递增的逻辑时钟之后，也就很容易决定哪些数据应该返回给用户。</li></ul><p>参考资料</p><p>1. <a href=http://andremouche.github.io/transaction/percolator.html>http://andremouche.github.io/transaction/percolator.html</a><br>2. <a href=https://research.google.com/pubs/pub36726.html>https://research.google.com/pubs/pub36726.html</a><br>3. <a href=https://github.com/xiaomi/themis>https://github.com/xiaomi/themis</a><br>4. <a href=http://nosqlmark.informatik.uni-hamburg.de/sdb2014/res/paper/Omid.pdf>http://nosqlmark.informatik.uni-hamburg.de/sdb2014/res/paper/Omid.pdf</a></p><p><strong>19. Achieving HBase Multi-Tenancy: RegionServer Groups and Favored Nodes</strong></p><p>这个talk介绍了Yahoos主导的两个重要feature：RegionServer Group和Favored Nodes。</p><p>1. RegionServer Group可以让指定表分布在某些RegionServer上，一定程度上实现了多租户之间的隔离，相比Hortonworks在微观层面对多租户所做出的努力（rpc队列分离、quota机制、请求优先级等），个人认为rsGroup在多租户管理方面效果可能更加明显。rsGroup是这次峰会比较明星的话题，包括滴滴等公司都作为主旨来介绍其在生产实践中的应用，可见已经被很多大厂所接受并实践，遗憾的是，该功能目前只在未发布的2.0.0里面包含，要想在当前版本中使用必须移植对应patch，因为功能的复杂性，这看起来并不是一件容易的事情。<br>2. Favored Nodes机制相对比较陌生，该功能允许HBase在region层面指定该region上所有文件分布在哪些DN之上，这个功能最大的好处是可以提升数据文件的本地率，试想，每个Region都知道文件存放的DN，迁移Region的时候就可以多考虑文件本地率进行迁移。</p><p><strong>20. Community-Driven Graphs With JanusGraph</strong></p><p>这个talk介绍了HBase应用的另一个重要领域－图数据库，图数据库在互联网业务中诸多方面（社交图谱分析、推荐分析）都有实际应用。比如大家可能比较熟悉的titan数据库就是一个典型的图数据库。而这个主题介绍的是另一个图数据库 - JanusGraph，PPT中分别对JanusGraph的架构、存储模型以及如何使用HBase作为后端存储进行了详细的分析。对Graph比较感兴趣的童鞋可以重点关注！</p><p>最后，附上PPT下载链接: <a href="https://pan.baidu.com/s/1mhLcafA#list/path=%2F">HBaseConWest2017</a></p><p>作者：胡争、范欣欣</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://openinx.github.io/tags/hbase/>HBase</a></li><li><a href=https://openinx.github.io/tags/apache/>Apache</a></li></ul><nav class=paginav><a class=prev href=https://openinx.github.io/posts/2016-06-21-hbase-balance/><span class=title>« Prev</span><br><span>HBase Region Balance实践</span></a>
<a class=next href=https://openinx.github.io/posts/2016-06-13-hbase-replicate-hlog/><span class=title>Next »</span><br><span>HBase回放Hlog顺序不一致的问题</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on twitter" href="https://twitter.com/intent/tweet/?text=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb&url=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f&hashtags=HBase%2cApache"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f&title=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb&summary=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb&source=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f&title=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on whatsapp" href="https://api.whatsapp.com/send?text=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb%20-%20https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share HBaseCon West 2017 Session解读 on telegram" href="https://telegram.me/share/url?text=HBaseCon%20West%202017%20Session%e8%a7%a3%e8%af%bb&url=https%3a%2f%2fopeninx.github.io%2fposts%2f2017-06-28-hbaseconwest2017%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://openinx.github.io>Openinx Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>