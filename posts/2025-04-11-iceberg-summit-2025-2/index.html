<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Iceberg Summit 2025 - Part 2 | Openinx Blog</title>
<meta name=keywords content="Iceberg,DataLake,Summit,BigData,LakeHouse"><meta name=description content="This is the second article of my thoughts for Iceberg Summit 2025 (Here is Part-1), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone."><meta name=author content="Zheng Hu"><link rel=canonical href=https://openinx.github.io/posts/2025-04-11-iceberg-summit-2025-2/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://openinx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://openinx.github.io/posts/2025-04-11-iceberg-summit-2025-2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B52L98PJKS"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B52L98PJKS")}</script><meta property="og:url" content="https://openinx.github.io/posts/2025-04-11-iceberg-summit-2025-2/"><meta property="og:site_name" content="Openinx Blog"><meta property="og:title" content="Iceberg Summit 2025 - Part 2"><meta property="og:description" content="This is the second article of my thoughts for Iceberg Summit 2025 (Here is Part-1), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-11T21:56:43-07:00"><meta property="article:modified_time" content="2025-04-11T21:56:43-07:00"><meta property="article:tag" content="Iceberg"><meta property="article:tag" content="DataLake"><meta property="article:tag" content="Summit"><meta property="article:tag" content="BigData"><meta property="article:tag" content="LakeHouse"><meta property="og:image" content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Iceberg Summit 2025 - Part 2"><meta name=twitter:description content="This is the second article of my thoughts for Iceberg Summit 2025 (Here is Part-1), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://openinx.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Iceberg Summit 2025 - Part 2","item":"https://openinx.github.io/posts/2025-04-11-iceberg-summit-2025-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Iceberg Summit 2025 - Part 2","name":"Iceberg Summit 2025 - Part 2","description":"This is the second article of my thoughts for Iceberg Summit 2025 (Here is Part-1), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone.\n","keywords":["Iceberg","DataLake","Summit","BigData","LakeHouse"],"articleBody":"This is the second article of my thoughts for Iceberg Summit 2025 (Here is Part-1), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone.\nAI + Data Lake As I mentioned in the previous article, every company or product in the Infra field should firmly grasp the trend of AI. In fact, many companies have done so and found a great positions in my opinion.\n“Data lake” usually refers to a centralized system used to store, process, and manage massive data. “Storage” usually includes Structured Data, Semi-Structured Data, and Unstructured Data; “Processing” usually includes common business scenarios such as ETL, BI analysis, feature engineering, data preprocessing, Model Training, and model post-training; “Management” usually includes dataset sharing, dataset permissions, dataset security, dataset lineage, and other collaboration-oriented things for different businesses.\nIceberg is now generally considered the de facto standard for Table Format in the field of Data Lake. However, having only one Iceberg Table Format is not enough to solve the complex dimensions of “storage”, “processing”, and “management” covered in the “general Data Lake”. For example, Iceberg is essentially a “storage” solution. Iceberg v1 \u0026 v2 are best suited for maintaining Structured Data, while iceberg v3 focuses on semi-Structured Data, which is relatively insufficient in unstructured scenarios. Compared with the concept of “Data lake” in the previous paragraph, I found that there are still many shortcomings in the Data lake solution around Iceberg, and it is these shortcomings that have inspired the innovation of various teams and brought new solutions.\nStorage This part is usually divided into two layers: “Format” and “Filesystem/ObjectStorage”.\nFirst, let’s talk about the Format part. The datasets involved in AI scenarios include Structured Data, Semi-Structured Data, and Unstructured Data. As mentioned earlier, Iceberg is currently not suitable for storing unstructured scenarios (including images, audio, video, etc.).\nFirstly, using Parquet to read and write large columns is inefficient and easy to OOM. Imagine a column storing video data, with each video ranging from 20MB to 100MB. Therefore, a 128MB RowGroup in Parquet can only store up to 6 rows of data. Such a small number of large field Rows is actually a disadvantage for Parquet column storage (such as high compression cost but low profit, and Pruning cannot hit because there are usually no column stats). The most critical issue is that due to the uncontrollable column byte size, Parquet writers are extremely prone to OOM, and stability is a huge challenge.\nSecondly, AI scenarios involve Random access, but Iceberg and Parquet do not provide a simple and easy-to-use interface. Why is this interface important? I have encountered two scenarios with clients: the first is that clients usually do JOINs between image datasets and text datasets to complete the cleaning or reconstruction of a mixed image and text data. JOINs essentially involve Random access to image data; the second is the Sample Shuffle scenario of AI training scenarios, which distributes data randomly, so that each concurrent training task obtains random data to promote the model to converge as soon as possible during the training process. Shuffle is essentially Random Access.\nFinally, as ML engineers deepen their understanding of AI datasets, they will continue to add new features. Corresponding to the dataset, they usually write new columns in batches. Currently, Iceberg needs to rewrite the entire dataset to complete this. This is unacceptable for huge datasets. Russell also mentioned this issue at the Iceberg Summit 2025 Panel Table . ByteDance maintains EB-level feature engineering data in Iceberg format internally, but they have changed the design of Iceberg internally. Please refer to the sharing [1] of Iceberg Summit 2024 for details.\nThe above issues were keenly captured by a data start-up company, which continuously iterated and implemented a usable solution. The company is called LanceDB, which creatively designed the lance file format (compared to Parquet) and a more semantically rich lance dataset format (compared to Iceberg) based on the lance file format. In many aspects (such as Transaction mechanism and Table file layout), the lancedb team absorbed the inspiration of iceberg and made similar designs, but with their own unique features. For example, boldly using Rust to write the underlying unified format library makes cross-language Format iteration faster; introducing the concept of RowID to provide random access APIs; introducing secondary indexes (B + Tree Index, FullText Index, Vector Index) at the dataset format level to facilitate more efficient data processing. Overall, I think their solution solves the three problems I listed at the beginning, which is very helpful for AI scenarios. Our team encountered this scenario last year and invested nearly a year in lance, which allowed us to gain two lance Committers and good customer revenue. Much thanks to the lancedb team. For more information on lancedb technology details, I suggest reading these materials [2], [3], [4]. At the Iceberg Summit 2025, I was pleased to see the LanceDB team working with the Iceberg community to integrate the lance format into Iceberg, giving Iceberg greater leadership in unstructured scenarios. Jack Ye has joined the LanceDB team and is a member of the Apache Iceberg PMC. I believe that with his help, Iceberg and Lance will have a better integration. Finally, other projects in the field of AI Dataset Format include Vortex [5], Nimble [6], Deeplake [7], and Streaming [8]. I hope to have the opportunity to analyze them separately in the future.\nLet’s talk about the “ObjectStorage/FileSystem” part. Some people may say that storage should be delegated to AWS S3 directly. Yes, Object Storage is very good, simple and easy to use, reliable data, and pay-as-you-go. But in fact, Only Table/Dataset Format and Object Storage are not enough, for AI scenarios.\nFirstly, in the pre-traning stage of AI scenarios, the data that needs to be cleaned may be tens or even hundreds of petabytes, and the number of files can reach billions. This requires extremely high throughput and metadata efficiency for Object Storage. On the one hand, the cost may be high; on the other hand, storage bottlenecks can easily lead to low GPU utilization. In fact, the GPU bill is much higher than the storage bill, and leaving the GPU unbusy is a waste.\nSecondly, AI scenarios heavily rely on the Python ecosystem, and many Python libraries use the Posix API to access underlying data. This requires that data on ObjectStorage can be accessed using the Posix API. Some people may say that with the Iceberg Table Format or Lance Format, Python can naturally skip the Posix API for access. Yes, but at least for now, it cannot be guaranteed that all data will be converted to Iceberg or Lance. Therefore, this Posix API is still necessary.\nFinally, the AI training scenario requires a random IO delay of sub-second, and the IOPS is very high. In addition, whether it is the checkpoint process of Model Training or the model deployment scenario, there will be a huge peak consumption of storage bandwidth. If the storage does not meet the requirements, it will lead to checkpoint failure or model deployment failure, resulting in greater GPU waste.\nAmong these start-up companies, Juicedata and Alluxio were one of the earliest to solve the issues. Among them, Juicedata may be the start-up company that has greatly benefited from this wave of AI. Many large model start-up companies in China are their customers, and it is said that their revenue growth has been very good in recent years. Davies, the founder of Juicedata, is a legendary geek engineer who worked at Databricks from 2014 to 2016. He believed that Databricks needed to develop a storage acceleration system based on s3. Databricks’ CTO Matei (author of Apache Spark) told him, “Storage is not something we are good at, so try not to touch it if possible.” So, Davies founded Juicedata in 2017. If Davies had stayed at Databricks, he would have been financially free, but excellent people would have succeeded in another field. Success is success, but from a technical perspective, I actually prefer the “transparent acceleration” technology direction, which can accelerate access to users’ existing massive S3 data. Of course, this solution is much more complex than the “non-transparent acceleration” solution (which Juicedata adopts), especially in terms of semantics and performance issues. We saw that in November 2023, AWS S3 released S3 express one zone, s3 mountpoint, and AI connectors, which essentially solve the above problems in a “transparent acceleration” way. The cost is 7x~ 8x of the regular S3 cost, but for AI scenarios, it is completely acceptable because the GPU is too expensive. MinIO has an acceleration solution called AIStor cache, but it seems to be an on-premise s3 closed-source acceleration solution. What people want more is a cloud-based acceleration solution for AWS S3.\nTo sum up, there are various entrepreneurial teams and large company teams that have achieved varying degrees of success in the field of “Data Lake Acceleration”. However, I guess there is still opportunity and space in this area. Because from the perspective of broad “ObjectStorage/FileSystem cache acceleration”, I have not yet seen a reliable and successful cloud vendor-neutral “transparent acceleration” solution. From the narrow perspective of “Data Lake Format Acceleration”, I believe that the solution of cache acceleration for Manifest Files in iceberg/lance and hot Pages in RowGroup/Fragment can achieve fine grain and extreme performance acceleration. For example, if a customer has a large amount of iceberg table data stored on AWS S3, the user only needs to activate a SaaS service to enjoy the effect of computing acceleration by 10x.\nProcessing In the data pprocessing stage of AI, Spark is usually used to complete CPU-intensive data processing tasks, while Ray has almost become the core framework for GPU-intensive data processing. Ray’s advantages are reflected in fine-grained control of GPU and CPU mixed resources, seamless native Python ecosystem to reuse GPU \u0026 AI ecology, etc. However, Ray has some shortcomings, such as not supporting Shuffle operation, not supporting SQL, etc.\nI met Daft’s booth at the venue and chatted with CEO Jay [9] for a long time. I finally got to the core problem that Daft wanted to solve. He believes that in the data processing scenario of MultiModal Machine Learning, a classic example is “semantic deduplication” - \u003e “clustering” - \u003e “model batch inference”. The first and second steps of this process are CPU-intensive and involve shuffling, which can only be handled by Spark; the third step is offline inference of small models, usually handled by Ray. These three steps involve processing across Spark and Ray engines, which brings several problems: firstly, AI/ML developers feel that Spark has a relatively high threshold and is relatively heavyweight; secondly, the data processed by Spark must be persisted to the object storage system or file system before it can be further processed by downstream Ray tasks. On the one hand, persistence has a high cost, and on the other hand, it requires configuring complex workflows to complete job scheduling across two engines; thirdly, completing this task requires AI/ML developers to master both Spark and Ray. Jay believes that the Daft solution is used to solve this problem. Daft supports SQL, Shuffle, and Join, and is native to Python, naturally running on the Ray framework. In short, he believes that Daft = Spark + Ray. Therefore, the above scenario only needs to be solved with Daft as a framework, which he believes is the core competitiveness of Daft.\nIn the past year or two, our team has been searching for the best computing framework for “AI MultiModal Machine Learning data processing” because we have seen the similar pain points of real customer scenarios. Daft may be a good candidate, but it will take some time to continue reviewing. Deepseek open-sourced Smallpond at Open Source Week, which is somewhat similar to Daft’s positioning [10]. It is a native Python ecosystem, supports SQL, supports Shuffle, and reuses Ray distributed framework to achieve CPU \u0026 GPU hybrid scheduling. The core weakness of Smallpond is that Shuffle relies too much on the internal high-performance parallel file system 3FS, and users need to switch between low-level API and SQL API, which leads to a slightly higher threshold. It may not be an open-source user-friendly solution that can be used immediately. Daft does better in these two aspects.\nFinally, I want to say that the Daft team is an extremely young team. Jay graduated from Cornel in 2018 and participated in the Lyft L5 project, which was acquired by Toyota. He founded Daft in 2022 and has been doing so until now. The other members of the booth are also young, and their conversations are full of enthusiasm and confidence. I am amazed that such a young team has such keen insight into the AI scene and has built the Daft project step by step with Rust + Python in 2 years. Daft even has a good landing in the internal scenes of Amazon and Together AI. I hope Daft’s development goes smoothly!\nManagement The management of data in Data Lake (Data Share, Permissions, Security, Lineage) usually relies on Catalog. I understand that there are currently at least four mainstream open source projects working on Iceberg-related Catalogs.\nApache Polaris: An open-source component led by Snowflake that focuses on making the Iceberg Catalog. Apache Gravitino: An open-source project contributed by Datastrato and donated to the Apache community. I learned that Xiaomi and Bilibili (China’s Youtube) have had good implementations on Gravitino. UnityCatalog: A Unity Catalog open-sourced by Databricks. LakeKeeper: Open sourced by a start-up company called Vakamo, they focus on open sourcing and products of the Iceberg Catalog. In summary, Polaris and LakeKeeper are a Catalog service that focuses on managing Iceberg, while Gravitino and UnityCatalog are positioned as a unified Catalog for Data + AI. Some people may think that Catalog is an easy thing to do, but I don’t think so. Because Catalog involves many troublesome issues such as cross-organizational customization standards, cross-project collaboration, revenue weighting, management permissions, SLAs, etc. Fortunately, Apache Iceberg not only defines the industry Table Format specification, but also defines the Catalog specification. Unfortunately, there is still a lack of industry standards in the non-Iceberg data management field. Whether it is AI computing engines or developers in the AI/ML field, they generally do not perceive the concept of Catalog. These pain points are opportunities in the Catalog field!\nStreaming + Data Lake In the field of Streaming + DataLake, I saw four companies at the venue: Confluent, Redpanda, StreamNative, and RisingWave.\nGenerally speaking, Confluent, Redpanda, and StreamNative are three companies that are focusing on DataLake from the perspective of Streaming Storage. Taking Confluent as an example to illustrate.\nFrom the perspective of Confluent, it manages a large amount of Kafka data, which ultimately needs to flow into AWS Redshift, Databricks Delta, Snowflake Tables, etc. From a product perspective, data flowing into any of them cannot be accessed by other services, which is a huge lock-in for users. From a business perspective, Kafka only buffers data from the last 7 days, and the massive historical data accumulated continuously ultimately flows away from Confluent, which is unfavorable for business and revenue. Custom Table Format is not suitable for Confluent because the lack of computing ecology will make users feel lock-in. Therefore, Confluent chose Iceberg and launched TableFlow [11]. This product allows Kafka’s Topic to become an open Iceberg table with one click, which can be managed within Confluent (this part is the new revenue added by Confluent) or in users’ buckets. “One click” solves many pain points in the middle (I explained the same pain points in 2021 [12]), including the cost of users managing Spark/Flink jobs themselves, the complexity of converting heterogeneous data sources such as Avro/parquet/JSON/Protobuf to Iceberg, pipeline interruptions caused by Schema evolution, Iceberg compaction and file management, etc. Overall, I believe that TableFlow is a win-win solution for both users and Confluent.\nRegarding RisingWave + DataLake, I consulted Wu Yingjun and found a slightly different approach. RisingWave is essentially a Streaming Database, including computation and storage. Its Postgres-compatible SQL can express both stream semantics and batch semantics. Users first create a Materilized View through SQL, and then query the Materialized View results in real-time through SQL to obtain millisecond-level query responses for complex data insights. The storage part of RisingWave can store both RisingWave built-in formats and Iceberg formats. The core advantage of storing Iceberg format is that users can easily ingest into the lake using PG SQL, and can also easily perform AP analysis through PG SQL. Finally, this data can be exposed to other Iceberg ecosystem computing engines for analysis. This is the RisingWave’s Streaming Lakehouse.\nHTAP + Data Lake The Sun Zhou and Chen Cheng teams I know are a startup team in the HTAP + DataLake direction, and the start-up company is called Mooncake. Before starting their start-up, both of them were principle engineers at SingleStore for many years, and SingleStore has always maintained a leading position in the HTAP field.\nThe Mooncake team implemented a Postgres extension service in Rust, which can subscribe to CDC data from Postgres OLTP tables and periodically convert CDC data to Apache Iceberg table format. In addition, they also implemented a pg_mooncake Postgres extension plugin, which allows Postgres to use Vectorized engine to quickly analyze Iceberg tables. By combining Postgres native capabilities, Rust server, and pg_mooncache, they built an HTAP solution around Postgres. This solution has several advantages that distinguish it from other solutions.\nFirst, an HTAP solution built around the native Postgres ecosystem. Secondly, the AP and TP sides ensure millisecond-level freshness and query latency. OLAP queries will join CDC incremental data and Iceberg table data in real time to achieve millisecond-level freshness. Third, Mookcache implements an Data Open HTAP solution based on the Apache Iceberg format. Mooncake’s positioning is very unique in the market, possibly the only open DataLake HTAP solution built around the Postgres ecosystem. I think the Mooncake team is awesome because they solved a difficult problem in a simple and elegant way.\nSummary At the Iceberg Summit 2025, I saw Iceberg successfully connect various open source and commercial ecosystems with its completely open Table Format, turning the world into a truly open Data Lake. From “Iceberg v3 and beyond” to here, I saw the infinite prospects of a MultiModal Machine Learning Data Lake that covers structured, semi-structured, and unstructured data for “Data + AI”. From various creative teams, I saw their enthusiasm, ideas, paths, and success. Anyway, this is just my partial view. There are more talks and scenorios at Iceberg Summit 2025. Welcome to explore together!\nReference https://www.youtube.com/watch?v=UPjr0qZ0-Do https://blog.lancedb.com/lance-v2/ https://lancedb.github.io/lance/format.html https://github.com/lancedb/lance-research/blob/main/file_2_1/paper/paper.pdf https://github.com/spiraldb/vortex https://github.com/facebookincubator/nimble https://github.com/activeloopai/deeplake https://github.com/mosaicml/streaming https://www.jaychia.com/ https://blog.getdaft.io/p/deepseek-smallpond-3fs-and-data-processing https://www.confluent.io/blog/latest-tableflow/ https://openinx.github.io/ppt/2021-04-25-flink-iceberg-shanghai-meetup.pdf ","wordCount":"3162","inLanguage":"en","image":"https://openinx.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-04-11T21:56:43-07:00","dateModified":"2025-04-11T21:56:43-07:00","author":{"@type":"Person","name":"Zheng Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://openinx.github.io/posts/2025-04-11-iceberg-summit-2025-2/"},"publisher":{"@type":"Organization","name":"Openinx Blog","logo":{"@type":"ImageObject","url":"https://openinx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://openinx.github.io/ accesskey=h title="openinx (Alt + H)">openinx</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://openinx.github.io/about/ title=About><span>About</span></a></li><li><a href=https://openinx.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://openinx.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://openinx.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://openinx.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Iceberg Summit 2025 - Part 2</h1><div class=post-meta><span title='2025-04-11 21:56:43 -0700 PDT'>April 11, 2025</span>&nbsp;·&nbsp;Zheng Hu</div></header><div class=post-content><p>This is the second article of my thoughts for Iceberg Summit 2025 (Here is <a href=/posts/2025-04-10-iceberg-summit-2025-1/>Part-1</a>), which is not limited to Iceberg but focuses on Data Lake. I am trying to share what these creative teams are doing in this field, why it is a problem, what the solution is, and how it will develop in the future. These insights come from various sharing, discussions, and debates, but are limited by my personal understanding. Anyway, I hope this article can inspire everyone.</p><h3 id=ai--data-lake>AI + Data Lake<a hidden class=anchor aria-hidden=true href=#ai--data-lake>#</a></h3><p>As I mentioned in the previous article, every company or product in the Infra field should firmly grasp the trend of AI. In fact, many companies have done so and found a great positions in my opinion.</p><p>&ldquo;<strong>Data lake</strong>&rdquo; usually refers to a centralized system used to store, process, and manage massive data. &ldquo;<strong>Storage</strong>&rdquo; usually includes Structured Data, Semi-Structured Data, and Unstructured Data; &ldquo;<strong>Processing</strong>&rdquo; usually includes common business scenarios such as ETL, BI analysis, feature engineering, data preprocessing, Model Training, and model post-training; &ldquo;<strong>Management</strong>&rdquo; usually includes dataset sharing, dataset permissions, dataset security, dataset lineage, and other collaboration-oriented things for different businesses.</p><p>Iceberg is now generally considered the de facto standard for <strong>Table Format</strong> in the field of Data Lake. However, having only one Iceberg Table Format is not enough to solve the complex dimensions of &ldquo;storage&rdquo;, &ldquo;processing&rdquo;, and &ldquo;management&rdquo; covered in the &ldquo;general Data Lake&rdquo;. For example, Iceberg is essentially a &ldquo;storage&rdquo; solution. Iceberg v1 & v2 are best suited for maintaining Structured Data, while iceberg v3 focuses on semi-Structured Data, which is relatively insufficient in unstructured scenarios. Compared with the concept of &ldquo;Data lake&rdquo; in the previous paragraph, I found that there are still many shortcomings in the Data lake solution around Iceberg, and it is these shortcomings that have inspired the innovation of various teams and brought new solutions.</p><h4 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h4><p>This part is usually divided into two layers: &ldquo;<strong>Format</strong>&rdquo; and &ldquo;<strong>Filesystem/ObjectStorage</strong>&rdquo;.</p><p><strong>First, let&rsquo;s talk about the Format part</strong>. The datasets involved in AI scenarios include Structured Data, Semi-Structured Data, and Unstructured Data. As mentioned earlier, Iceberg is currently not suitable for storing unstructured scenarios (including images, audio, video, etc.).</p><p>Firstly, using Parquet to read and write large columns is inefficient and easy to OOM. Imagine a column storing video data, with each video ranging from 20MB to 100MB. Therefore, a 128MB RowGroup in Parquet can only store up to 6 rows of data. Such a small number of large field Rows is actually a disadvantage for Parquet column storage (such as high compression cost but low profit, and Pruning cannot hit because there are usually no column stats). The most critical issue is that due to the uncontrollable column byte size, Parquet writers are extremely prone to OOM, and stability is a huge challenge.</p><p>Secondly, AI scenarios involve Random access, but Iceberg and Parquet do not provide a simple and easy-to-use interface. Why is this interface important? I have encountered two scenarios with clients: the first is that clients usually <strong>do JOINs between image datasets and text datasets</strong> to complete the cleaning or reconstruction of a mixed image and text data. JOINs essentially involve Random access to image data; the second is <strong>the Sample Shuffle scenario</strong> of AI training scenarios, which distributes data randomly, so that each concurrent training task obtains random data to promote the model to converge as soon as possible during the training process. Shuffle is essentially Random Access.</p><p>Finally, as ML engineers deepen their understanding of AI datasets, they will continue to add new features. Corresponding to the dataset, they usually write new columns in batches. Currently, Iceberg needs to rewrite the entire dataset to complete this. This is unacceptable for huge datasets. <a href=https://www.linkedin.com/in/russell-spitzer-5697328/>Russell</a> also mentioned this issue at the <strong>Iceberg Summit 2025 Panel Table</strong> . ByteDance maintains EB-level feature engineering data in Iceberg format internally, but they have changed the design of Iceberg internally. Please refer to the sharing [1] of Iceberg Summit 2024 for details.</p><p>The above issues were keenly captured by a data start-up company, which continuously iterated and implemented a usable solution. The company is called <a href=https://lancedb.com/>LanceDB</a>, which creatively designed the lance file format (compared to Parquet) and a more semantically rich lance dataset format (compared to Iceberg) based on the lance file format. In many aspects (such as Transaction mechanism and Table file layout), the lancedb team absorbed the inspiration of iceberg and made similar designs, but with their own unique features. For example, boldly using Rust to write the underlying unified format library makes cross-language Format iteration faster; introducing the concept of <strong>RowID</strong> to provide random access APIs; introducing secondary indexes (B + Tree Index, FullText Index, Vector Index) at the dataset format level to facilitate more efficient data processing. Overall, I think their solution solves the three problems I listed at the beginning, which is very helpful for AI scenarios. Our team encountered this scenario last year and invested nearly a year in lance, which allowed us to gain two lance Committers and good customer revenue. Much thanks to the lancedb team. For more information on lancedb technology details, I suggest reading these materials [2], [3], [4]. At the Iceberg Summit 2025, I was pleased to see the LanceDB team working with the Iceberg community to integrate the lance format into Iceberg, giving Iceberg greater leadership in unstructured scenarios. <a href=https://www.linkedin.com/in/yezhaoqin/>Jack Ye</a> has joined the LanceDB team and is a member of the Apache Iceberg PMC. I believe that with his help, Iceberg and Lance will have a better integration. Finally, other projects in the field of AI Dataset Format include Vortex [5], Nimble [6], Deeplake [7], and Streaming [8]. I hope to have the opportunity to analyze them separately in the future.</p><p><strong>Let&rsquo;s talk about the &ldquo;ObjectStorage/FileSystem&rdquo; part</strong>. Some people may say that storage should be delegated to AWS S3 directly. Yes, Object Storage is very good, simple and easy to use, reliable data, and pay-as-you-go. But in fact, Only Table/Dataset Format and Object Storage are not enough, for AI scenarios.</p><p>Firstly, in the pre-traning stage of AI scenarios, the data that needs to be cleaned may be tens or even hundreds of petabytes, and the number of files can reach billions. This requires extremely high throughput and metadata efficiency for Object Storage. On the one hand, the cost may be high; on the other hand, storage bottlenecks can easily lead to low GPU utilization. In fact, the GPU bill is much higher than the storage bill, and leaving the GPU unbusy is a waste.</p><p>Secondly, AI scenarios heavily rely on the Python ecosystem, and many Python libraries use the Posix API to access underlying data. This requires that data on ObjectStorage can be accessed using the Posix API. Some people may say that with the Iceberg Table Format or Lance Format, Python can naturally skip the Posix API for access. Yes, but at least for now, it cannot be guaranteed that all data will be converted to Iceberg or Lance. Therefore, this Posix API is still necessary.</p><p>Finally, the AI training scenario requires a random IO delay of sub-second, and the IOPS is very high. In addition, whether it is the checkpoint process of Model Training or the model deployment scenario, there will be a huge peak consumption of storage bandwidth. If the storage does not meet the requirements, it will lead to checkpoint failure or model deployment failure, resulting in greater GPU waste.</p><p>Among these start-up companies, <a href=https://github.com/juicedata>Juicedata</a> and <a href=https://www.alluxio.io/>Alluxio</a> were one of the earliest to solve the issues. Among them, Juicedata may be the start-up company that has greatly benefited from this wave of AI. Many large model start-up companies in China are their customers, and it is said that their revenue growth has been very good in recent years. <a href=https://www.linkedin.com/in/daviesliu/>Davies</a>, the founder of Juicedata, is a legendary geek engineer who worked at Databricks from 2014 to 2016. He believed that Databricks needed to develop a storage acceleration system based on s3. Databricks&rsquo; CTO Matei (author of Apache Spark) told him, &ldquo;Storage is not something we are good at, so try not to touch it if possible.&rdquo; So, Davies founded Juicedata in 2017. If Davies had stayed at Databricks, he would have been financially free, but excellent people would have succeeded in another field. Success is success, but from a technical perspective, I actually prefer the &ldquo;<strong>transparent acceleration</strong>&rdquo; technology direction, which can accelerate access to users&rsquo; existing massive S3 data. Of course, this solution is much more complex than the &ldquo;<strong>non-transparent acceleration</strong>&rdquo; solution (which Juicedata adopts), especially in terms of semantics and performance issues. We saw that in November 2023, AWS S3 released <a href=https://aws.amazon.com/s3/storage-classes/express-one-zone/>S3 express one zone</a>, <a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/mountpoint.html>s3 mountpoint</a>, and AI connectors, which essentially solve the above problems in a &ldquo;transparent acceleration&rdquo; way. The cost is 7x~ 8x of the regular S3 cost, but for AI scenarios, it is completely acceptable because the GPU is too expensive. MinIO has an acceleration solution called <a href=https://min.io/product/aistor/object-store-cache>AIStor cache</a>, but it seems to be an on-premise s3 closed-source acceleration solution. What people want more is a cloud-based acceleration solution for AWS S3.</p><p>To sum up, there are various entrepreneurial teams and large company teams that have achieved varying degrees of success in the field of &ldquo;Data Lake Acceleration&rdquo;. However, I guess there is still opportunity and space in this area. Because from the perspective of broad &ldquo;ObjectStorage/FileSystem cache acceleration&rdquo;, I have not yet seen a reliable and successful cloud vendor-neutral &ldquo;transparent acceleration&rdquo; solution. From the narrow perspective of &ldquo;Data Lake Format Acceleration&rdquo;, I believe that the solution of cache acceleration for Manifest Files in iceberg/lance and hot Pages in RowGroup/Fragment can achieve fine grain and extreme performance acceleration. For example, if a customer has a large amount of iceberg table data stored on AWS S3, the user only needs to activate a SaaS service to enjoy the effect of computing acceleration by 10x.</p><h4 id=processing>Processing<a hidden class=anchor aria-hidden=true href=#processing>#</a></h4><p>In the data pprocessing stage of AI, Spark is usually used to complete CPU-intensive data processing tasks, while Ray has almost become the core framework for GPU-intensive data processing. Ray&rsquo;s advantages are reflected in fine-grained control of GPU and CPU mixed resources, seamless native Python ecosystem to reuse GPU & AI ecology, etc. However, Ray has some shortcomings, such as not supporting Shuffle operation, not supporting SQL, etc.</p><p>I met <a href=https://www.getdaft.io/>Daft</a>&rsquo;s booth at the venue and chatted with CEO Jay [9] for a long time. I finally got to the core problem that Daft wanted to solve. He believes that in the data processing scenario of MultiModal Machine Learning, a classic example is &ldquo;semantic deduplication&rdquo; - > &ldquo;clustering&rdquo; - > &ldquo;model batch inference&rdquo;. The first and second steps of this process are CPU-intensive and involve shuffling, which can only be handled by Spark; the third step is offline inference of small models, usually handled by Ray. These three steps involve processing across Spark and Ray engines, which brings several problems: firstly, AI/ML developers feel that Spark has a relatively high threshold and is relatively heavyweight; secondly, the data processed by Spark must be persisted to the object storage system or file system before it can be further processed by downstream Ray tasks. On the one hand, persistence has a high cost, and on the other hand, it requires configuring complex workflows to complete job scheduling across two engines; thirdly, completing this task requires AI/ML developers to master both Spark and Ray. Jay believes that the Daft solution is used to solve this problem. Daft supports SQL, Shuffle, and Join, and is native to Python, naturally running on the Ray framework. In short, he believes that Daft = Spark + Ray. Therefore, the above scenario only needs to be solved with Daft as a framework, which he believes is the core competitiveness of Daft.</p><p>In the past year or two, our team has been searching for the best computing framework for &ldquo;AI MultiModal Machine Learning data processing&rdquo; because we have seen the similar pain points of real customer scenarios. Daft may be a good candidate, but it will take some time to continue reviewing. Deepseek open-sourced Smallpond at Open Source Week, which is somewhat similar to Daft&rsquo;s positioning [10]. It is a native Python ecosystem, supports SQL, supports Shuffle, and reuses Ray distributed framework to achieve CPU & GPU hybrid scheduling. The core weakness of Smallpond is that Shuffle relies too much on the internal high-performance parallel file system 3FS, and users need to switch between low-level API and SQL API, which leads to a slightly higher threshold. It may not be an open-source user-friendly solution that can be used immediately. Daft does better in these two aspects.</p><p>Finally, I want to say that the Daft team is an extremely young team. Jay graduated from Cornel in 2018 and participated in the Lyft L5 project, which was acquired by Toyota. He founded Daft in 2022 and has been doing so until now. The other members of the booth are also young, and their conversations are full of enthusiasm and confidence. I am amazed that such a young team has such keen insight into the AI scene and has built the Daft project step by step with Rust + Python in 2 years. Daft even has a good landing in the internal scenes of Amazon and Together AI. I hope Daft&rsquo;s development goes smoothly!</p><h4 id=management>Management<a hidden class=anchor aria-hidden=true href=#management>#</a></h4><p>The management of data in Data Lake (Data Share, Permissions, Security, Lineage) usually relies on Catalog. I understand that there are currently at least four mainstream open source projects working on Iceberg-related Catalogs.</p><ul><li><a href=https://polaris.apache.org/>Apache Polaris</a>: An open-source component led by Snowflake that focuses on making the Iceberg Catalog.</li><li><a href=https://gravitino.apache.org/>Apache Gravitino</a>: An open-source project contributed by Datastrato and donated to the Apache community. I learned that Xiaomi and Bilibili (China&rsquo;s Youtube) have had good implementations on Gravitino.</li><li><a href=https://www.unitycatalog.io/>UnityCatalog</a>: A Unity Catalog open-sourced by Databricks.</li><li><a href=https://docs.lakekeeper.io/>LakeKeeper</a>: Open sourced by a start-up company called Vakamo, they focus on open sourcing and products of the Iceberg Catalog.</li></ul><p>In summary, Polaris and LakeKeeper are a Catalog service that focuses on managing Iceberg, while Gravitino and UnityCatalog are positioned as a unified Catalog for Data + AI. Some people may think that Catalog is an easy thing to do, but I don&rsquo;t think so. Because Catalog involves many troublesome issues such as <strong>cross-organizational customization standards</strong>, <strong>cross-project collaboration</strong>, <strong>revenue weighting</strong>, <strong>management permissions</strong>, <strong>SLAs</strong>, etc. Fortunately, Apache Iceberg not only defines the industry Table Format specification, but also defines the Catalog specification. Unfortunately, there is still a lack of industry standards in the non-Iceberg data management field. Whether it is AI computing engines or developers in the AI/ML field, they generally do not perceive the concept of Catalog. These pain points are opportunities in the Catalog field!</p><h3 id=streaming--data-lake>Streaming + Data Lake<a hidden class=anchor aria-hidden=true href=#streaming--data-lake>#</a></h3><p>In the field of Streaming + DataLake, I saw four companies at the venue: Confluent, Redpanda, StreamNative, and RisingWave.</p><p>Generally speaking, Confluent, Redpanda, and StreamNative are three companies that are focusing on DataLake from the perspective of Streaming Storage. Taking Confluent as an example to illustrate.</p><p>From the perspective of Confluent, it manages a large amount of Kafka data, which ultimately needs to flow into AWS Redshift, Databricks Delta, Snowflake Tables, etc. From a product perspective, data flowing into any of them cannot be accessed by other services, which is a huge lock-in for users. From a business perspective, Kafka only buffers data from the last 7 days, and the massive historical data accumulated continuously ultimately flows away from Confluent, which is unfavorable for business and revenue. Custom Table Format is not suitable for Confluent because the lack of computing ecology will make users feel lock-in. Therefore, Confluent chose Iceberg and launched TableFlow [11]. This product allows Kafka&rsquo;s Topic to become an open Iceberg table with one click, which can be managed within Confluent (this part is the new revenue added by Confluent) or in users&rsquo; buckets. &ldquo;<strong>One click</strong>&rdquo; solves many pain points in the middle (I explained the same pain points in 2021 [12]), including <strong>the cost of users managing Spark/Flink jobs themselves</strong>, <strong>the complexity of converting heterogeneous data sources such as Avro/parquet/JSON/Protobuf to Iceberg</strong>, <strong>pipeline interruptions caused by Schema evolution</strong>, <strong>Iceberg compaction and file management</strong>, etc. Overall, I believe that TableFlow is a win-win solution for both users and Confluent.</p><p>Regarding RisingWave + DataLake, I consulted Wu Yingjun and found a slightly different approach. RisingWave is essentially a Streaming Database, including computation and storage. Its Postgres-compatible SQL can express both stream semantics and batch semantics. Users first create a <strong>Materilized View</strong> through SQL, and then query the Materialized View results in real-time through SQL to obtain millisecond-level query responses for complex data insights. The storage part of RisingWave can store both RisingWave built-in formats and Iceberg formats. The core advantage of storing Iceberg format is that users can easily ingest into the lake using PG SQL, and can also easily perform AP analysis through PG SQL. Finally, this data can be exposed to other Iceberg ecosystem computing engines for analysis. This is the RisingWave&rsquo;s <strong>Streaming Lakehouse</strong>.</p><h3 id=htap--data-lake>HTAP + Data Lake<a hidden class=anchor aria-hidden=true href=#htap--data-lake>#</a></h3><p>The <a href=https://www.linkedin.com/in/zhou-sun-b7970460/>Sun Zhou</a> and <a href=https://www.linkedin.com/in/cheng-ch/>Chen Cheng</a> teams I know are a startup team in the HTAP + DataLake direction, and the start-up company is called <a href=https://www.mooncake.dev/>Mooncake</a>. Before starting their start-up, both of them were principle engineers at SingleStore for many years, and SingleStore has always maintained a leading position in the HTAP field.</p><p>The Mooncake team implemented a Postgres extension service in Rust, which can subscribe to CDC data from Postgres OLTP tables and periodically convert CDC data to Apache Iceberg table format. In addition, they also implemented a pg_mooncake Postgres extension plugin, which allows Postgres to use Vectorized engine to quickly analyze Iceberg tables. By combining <strong>Postgres native capabilities</strong>, <strong>Rust server</strong>, and <strong>pg_mooncache</strong>, they built an HTAP solution around Postgres. This solution has several advantages that distinguish it from other solutions.</p><ul><li>First, an HTAP solution built around the native Postgres ecosystem.</li><li>Secondly, the AP and TP sides ensure millisecond-level freshness and query latency. OLAP queries will join CDC incremental data and Iceberg table data in real time to achieve millisecond-level freshness.</li><li>Third, Mookcache implements an <strong>Data Open</strong> HTAP solution based on the Apache Iceberg format.</li></ul><p>Mooncake&rsquo;s positioning is very unique in the market, possibly the only open DataLake HTAP solution built around the Postgres ecosystem. I think the Mooncake team is awesome because they solved a difficult problem in a simple and elegant way.</p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>At the Iceberg Summit 2025, I saw Iceberg successfully connect various open source and commercial ecosystems with its completely open Table Format, turning the world into a truly open Data Lake. From &ldquo;Iceberg v3 and beyond&rdquo; to here, I saw the infinite prospects of a MultiModal Machine Learning Data Lake that covers structured, semi-structured, and unstructured data for &ldquo;Data + AI&rdquo;. From various creative teams, I saw their enthusiasm, ideas, paths, and success. Anyway, this is just my partial view. There are more talks and scenorios at Iceberg Summit 2025. Welcome to explore together!</p><h3 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h3><ol><li><a href="https://www.youtube.com/watch?v=UPjr0qZ0-Do">https://www.youtube.com/watch?v=UPjr0qZ0-Do</a></li><li><a href=https://blog.lancedb.com/lance-v2/>https://blog.lancedb.com/lance-v2/</a></li><li><a href=https://lancedb.github.io/lance/format.html>https://lancedb.github.io/lance/format.html</a></li><li><a href=https://github.com/lancedb/lance-research/blob/main/file_2_1/paper/paper.pdf>https://github.com/lancedb/lance-research/blob/main/file_2_1/paper/paper.pdf</a></li><li><a href=https://github.com/spiraldb/vortex>https://github.com/spiraldb/vortex</a></li><li><a href=https://github.com/facebookincubator/nimble>https://github.com/facebookincubator/nimble</a></li><li><a href=https://github.com/activeloopai/deeplake>https://github.com/activeloopai/deeplake</a></li><li><a href=https://github.com/mosaicml/streaming>https://github.com/mosaicml/streaming</a></li><li><a href=https://www.jaychia.com/>https://www.jaychia.com/</a></li><li><a href=https://blog.getdaft.io/p/deepseek-smallpond-3fs-and-data-processing>https://blog.getdaft.io/p/deepseek-smallpond-3fs-and-data-processing</a></li><li><a href=https://www.confluent.io/blog/latest-tableflow/>https://www.confluent.io/blog/latest-tableflow/</a></li><li><a href=https://openinx.github.io/ppt/2021-04-25-flink-iceberg-shanghai-meetup.pdf>https://openinx.github.io/ppt/2021-04-25-flink-iceberg-shanghai-meetup.pdf</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://openinx.github.io/tags/iceberg/>Iceberg</a></li><li><a href=https://openinx.github.io/tags/datalake/>DataLake</a></li><li><a href=https://openinx.github.io/tags/summit/>Summit</a></li><li><a href=https://openinx.github.io/tags/bigdata/>BigData</a></li><li><a href=https://openinx.github.io/tags/lakehouse/>LakeHouse</a></li></ul><nav class=paginav><a class=next href=https://openinx.github.io/posts/2025-04-10-iceberg-summit-2025-1/><span class=title>Next »</span><br><span>Iceberg Summit 2025 - Part 1</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://openinx.github.io/>Openinx Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>